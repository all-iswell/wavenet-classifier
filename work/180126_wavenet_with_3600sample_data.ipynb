{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeatda/anaconda3/envs/proj3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.client import timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../_data/combined_csv/180126_3600samp_cry386_laugh703_with_ids_mu_law.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter_width          2\n",
      "sample_rate           22050\n",
      "dilations             [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
      "residual_channels     16\n",
      "dilation_channels     16\n",
      "quantization_channels 256\n",
      "skip_channels         16\n",
      "use_biases            True\n",
      "scalar_input          False\n",
      "initial_filter_width  32\n"
     ]
    }
   ],
   "source": [
    "wavenet_params = {\n",
    "    \"filter_width\": 2,\n",
    "    \"sample_rate\": 22050,\n",
    "    \"dilations\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512,\n",
    "                  1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "    \"residual_channels\": 16,\n",
    "    \"dilation_channels\": 16,\n",
    "    \"quantization_channels\": 256,\n",
    "    \"skip_channels\": 16,\n",
    "    \"use_biases\": True,\n",
    "    \"scalar_input\": False,\n",
    "    \"initial_filter_width\": 32\n",
    "}\n",
    "\n",
    "for key in wavenet_params.keys():\n",
    "    exec('{} = wavenet_params[\\'{}\\']'.format(key, key))\n",
    "    print('{:21}'.format(key), eval(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for WaveNetModel class\n",
    "\n",
    "def create_variable(name, shape):\n",
    "    '''Create a convolution filter variable with the specified name and shape,\n",
    "    and initialize it using Xavier initialition.'''\n",
    "    initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    variable = tf.Variable(initializer(shape=shape), name=name)\n",
    "    return variable\n",
    "\n",
    "\n",
    "def create_embedding_table(name, shape):\n",
    "    if shape[0] == shape[1]:\n",
    "        # Make a one-hot encoding as the initial value.\n",
    "        initial_val = np.identity(n=shape[0], dtype=np.float32)\n",
    "        return tf.Variable(initial_val, name=name)\n",
    "    else:\n",
    "        return create_variable(name, shape)\n",
    "\n",
    "\n",
    "def create_bias_variable(name, shape):\n",
    "    '''Create a bias variable with the specified name and shape and initialize\n",
    "    it to zero.'''\n",
    "    initializer = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    return tf.Variable(initializer(shape=shape), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time_to_batch, batch_to_time, causal_conv\n",
    "\n",
    "def time_to_batch(value, dilation, name=None):\n",
    "    with tf.name_scope('time_to_batch'):\n",
    "        shape = tf.shape(value)\n",
    "        pad_elements = dilation - 1 - (shape[1] + dilation - 1) % dilation\n",
    "        padded = tf.pad(value, [[0, 0], [0, pad_elements], [0, 0]])\n",
    "        reshaped = tf.reshape(padded, [-1, dilation, shape[2]])\n",
    "        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n",
    "        return tf.reshape(transposed, [shape[0] * dilation, -1, shape[2]])\n",
    "\n",
    "\n",
    "def batch_to_time(value, dilation, name=None):\n",
    "    with tf.name_scope('batch_to_time'):\n",
    "        shape = tf.shape(value)\n",
    "        prepared = tf.reshape(value, [dilation, -1, shape[2]])\n",
    "        transposed = tf.transpose(prepared, perm=[1, 0, 2])\n",
    "        return tf.reshape(transposed,\n",
    "                          [tf.div(shape[0], dilation), -1, shape[2]])\n",
    "\n",
    "\n",
    "def causal_conv(value, filter_, dilation, name='causal_conv'):\n",
    "    with tf.name_scope(name):\n",
    "        filter_width = tf.shape(filter_)[0]\n",
    "        if dilation > 1:\n",
    "            transformed = time_to_batch(value, dilation)\n",
    "            conv = tf.nn.conv1d(transformed, filter_, stride=1,\n",
    "                                padding='VALID')\n",
    "            restored = batch_to_time(conv, dilation)\n",
    "        else:\n",
    "            restored = tf.nn.conv1d(value, filter_, stride=1, padding='VALID')\n",
    "        # Remove excess elements at the end.\n",
    "        out_width = tf.shape(value)[1] - (filter_width - 1) * dilation\n",
    "        result = tf.slice(restored,\n",
    "                          [0, 0, 0],\n",
    "                          [-1, out_width, -1])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## scalar_input option does not work now\n",
    "\n",
    "class WaveNetModel(object):\n",
    "    '''WaveNet model modified for binary classification.\n",
    "    Modified by John Choi (isnbh0)\n",
    "\n",
    "    Default parameters:\n",
    "        dilations = [2**i for i in range(10)] * 2\n",
    "        filter_width = 2  # Convolutions just use 2 samples.\n",
    "        residual_channels = 16  # Not specified in the paper.\n",
    "        dilation_channels = 16  # Not specified in the paper.\n",
    "        skip_channels = 16      # Not specified in the paper.\n",
    "        net = WaveNetModel(batch_size, dilations, filter_width,\n",
    "                           residual_channels, dilation_channels,\n",
    "                           skip_channels)\n",
    "        loss = net.loss(input_batch)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 dilations,\n",
    "                 filter_width,\n",
    "                 residual_channels,\n",
    "                 dilation_channels,\n",
    "                 skip_channels,\n",
    "                 quantization_channels=2**8,\n",
    "                 use_biases=False,\n",
    "                 scalar_input=False,\n",
    "                 initial_filter_width=32,\n",
    "                 histograms=False):\n",
    "        '''Initializes the WaveNet model.\n",
    "\n",
    "        Args:\n",
    "            batch_size: audio files per batch (recommended: 1).\n",
    "            dilations: A list with the dilation factor for each layer.\n",
    "            filter_width: The samples that are included in each convolution,\n",
    "                after dilating.\n",
    "            residual_channels: # filters to learn for the residual.\n",
    "            dilation_channels: # filters to learn for the dilated convolution.\n",
    "            skip_channels: # filters to learn that contribute to the\n",
    "                quantized softmax output.\n",
    "            quantization_channels: # amplitude values to use for audio\n",
    "                quantization and the corresponding one-hot encoding.\n",
    "                Default: 256 (8-bit quantization).\n",
    "            use_biases: Whether to add a bias layer to each convolution.\n",
    "                Default: False.\n",
    "            scalar_input: Whether to use the quantized waveform directly as\n",
    "            |   input to the network instead of one-hot encoding it.\n",
    "            |   Default: False.\n",
    "            *-initial_filter_width: The width of the initial filter of the\n",
    "                convolution applied to the scalar input. This is only relevant\n",
    "                if scalar_input=True.\n",
    "            histograms: Whether to store histograms in the summary.\n",
    "                Default: False.\n",
    "\n",
    "\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.residual_channels = residual_channels\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.skip_channels = skip_channels\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.histograms = histograms\n",
    "\n",
    "        self.receptive_field = WaveNetModel.calculate_receptive_field(\n",
    "            self.filter_width, self.dilations, self.scalar_input,\n",
    "            self.initial_filter_width)\n",
    "        self.variables = self._create_variables()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_receptive_field(filter_width, dilations, scalar_input,\n",
    "                                  initial_filter_width):\n",
    "        receptive_field = (filter_width - 1) * sum(dilations) + 1\n",
    "        if scalar_input:\n",
    "            receptive_field += initial_filter_width - 1\n",
    "        else:\n",
    "            receptive_field += filter_width - 1\n",
    "        return receptive_field\n",
    "\n",
    "    def _create_variables(self):\n",
    "        '''This function creates all variables used by the network.\n",
    "        This allows us to share them between multiple calls to the loss\n",
    "        function and generation function.'''\n",
    "\n",
    "        var = dict()\n",
    "\n",
    "        with tf.variable_scope('wavenet'):\n",
    "            with tf.variable_scope('causal_layer'):\n",
    "                layer = dict()\n",
    "                if self.scalar_input:\n",
    "                    initial_channels = 1\n",
    "                    initial_filter_width = self.initial_filter_width\n",
    "                else:\n",
    "                    initial_channels = self.quantization_channels\n",
    "                    initial_filter_width = self.filter_width\n",
    "                layer['filter'] = create_variable(\n",
    "                    'filter',\n",
    "                    [initial_filter_width,\n",
    "                     initial_channels,\n",
    "                     self.residual_channels])\n",
    "                var['causal_layer'] = layer\n",
    "\n",
    "            var['dilated_stack'] = list()\n",
    "            with tf.variable_scope('dilated_stack'):\n",
    "                for i, dilation in enumerate(self.dilations):\n",
    "                    with tf.variable_scope('layer{}'.format(i)):\n",
    "                        current = dict()\n",
    "                        current['filter'] = create_variable(\n",
    "                            'filter',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['gate'] = create_variable(\n",
    "                            'gate',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['dense'] = create_variable(\n",
    "                            'dense',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.residual_channels])\n",
    "                        current['skip'] = create_variable(\n",
    "                            'skip',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.skip_channels])\n",
    "\n",
    "\n",
    "\n",
    "                        if self.use_biases:\n",
    "                            current['filter_bias'] = create_bias_variable(\n",
    "                                'filter_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['gate_bias'] = create_bias_variable(\n",
    "                                'gate_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['dense_bias'] = create_bias_variable(\n",
    "                                'dense_bias',\n",
    "                                [self.residual_channels])\n",
    "                            current['skip_bias'] = create_bias_variable(\n",
    "                                'slip_bias',\n",
    "                                [self.skip_channels])\n",
    "\n",
    "                        var['dilated_stack'].append(current)\n",
    "\n",
    "            with tf.variable_scope('postprocessing'):\n",
    "                current = dict()\n",
    "                current['postprocess1'] = create_variable(\n",
    "                    'postprocess1',\n",
    "                    [1, self.skip_channels, self.skip_channels])\n",
    "                current['postprocess2'] = create_variable(\n",
    "                    'postprocess2',\n",
    "                    [1, self.skip_channels, 1])  ## returns scalar value\n",
    "                if self.use_biases:\n",
    "                    current['postprocess1_bias'] = create_bias_variable(\n",
    "                        'postprocess1_bias',\n",
    "                        [self.skip_channels])\n",
    "                    current['postprocess2_bias'] = create_bias_variable(\n",
    "                        'postprocess2_bias',\n",
    "                        [1])\n",
    "                var['postprocessing'] = current\n",
    "\n",
    "        return var\n",
    "\n",
    "    def _create_causal_layer(self, input_batch):\n",
    "        '''Creates a single causal convolution layer.\n",
    "\n",
    "        The layer can change the number of channels.\n",
    "        '''\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            return causal_conv(input_batch, weights_filter, 1)\n",
    "\n",
    "    def _create_dilation_layer(self, input_batch, layer_index, dilation,\n",
    "                               output_width):\n",
    "        '''Creates a single causal dilated convolution layer.\n",
    "\n",
    "        Args:\n",
    "             input_batch: Input to the dilation layer.\n",
    "             layer_index: Integer indicating which layer this is.\n",
    "             dilation: Integer specifying the dilation size.\n",
    "\n",
    "        The layer contains a gated filter that connects to dense output\n",
    "        and to a skip connection:\n",
    "\n",
    "               |-> [gate]   -|        |-> 1x1 conv -> skip output\n",
    "               |             |-> (*) -|\n",
    "        input -|-> [filter] -|        |-> 1x1 conv -|\n",
    "               |                                    |-> (+) -> dense output\n",
    "               |------------------------------------|\n",
    "\n",
    "        Where `[gate]` and `[filter]` are causal convolutions with a\n",
    "        non-linear activation at the output. Biases and global conditioning\n",
    "        are omitted due to the limits of ASCII art.\n",
    "\n",
    "        '''\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "\n",
    "        conv_filter = causal_conv(input_batch, weights_filter, dilation)\n",
    "        conv_gate = causal_conv(input_batch, weights_gate, dilation)\n",
    "\n",
    "\n",
    "        if self.use_biases:\n",
    "            filter_bias = variables['filter_bias']\n",
    "            gate_bias = variables['gate_bias']\n",
    "            conv_filter = tf.add(conv_filter, filter_bias)\n",
    "            conv_gate = tf.add(conv_gate, gate_bias)\n",
    "\n",
    "        out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\n",
    "\n",
    "        # The 1x1 conv to produce the residual output\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.nn.conv1d(\n",
    "            out, weights_dense, stride=1, padding=\"SAME\", name=\"dense\")\n",
    "\n",
    "        # The 1x1 conv to produce the skip output\n",
    "        skip_cut = tf.shape(out)[1] - output_width\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.nn.conv1d(\n",
    "            out_skip, weights_skip, stride=1, padding=\"SAME\", name=\"skip\")\n",
    "\n",
    "        if self.use_biases:\n",
    "            dense_bias = variables['dense_bias']\n",
    "            skip_bias = variables['skip_bias']\n",
    "            transformed = transformed + dense_bias\n",
    "            skip_contribution = skip_contribution + skip_bias\n",
    "\n",
    "        if self.histograms:\n",
    "            layer = 'layer{}'.format(layer_index)\n",
    "            tf.summary.histogram(layer + '_filter', weights_filter)\n",
    "            tf.summary.histogram(layer + '_gate', weights_gate)\n",
    "            tf.summary.histogram(layer + '_dense', weights_dense)\n",
    "            tf.summary.histogram(layer + '_skip', weights_skip)\n",
    "            if self.use_biases:\n",
    "                tf.summary.histogram(layer + '_biases_filter', filter_bias)\n",
    "                tf.summary.histogram(layer + '_biases_gate', gate_bias)\n",
    "                tf.summary.histogram(layer + '_biases_dense', dense_bias)\n",
    "                tf.summary.histogram(layer + '_biases_skip', skip_bias)\n",
    "\n",
    "        input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\n",
    "        input_batch = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _generator_conv(self, input_batch, state_batch, weights):\n",
    "        '''Perform convolution for a single convolutional processing step.'''\n",
    "        # TODO generalize to filter_width > 2\n",
    "        past_weights = weights[0, :, :]\n",
    "        curr_weights = weights[1, :, :]\n",
    "        output = tf.matmul(state_batch, past_weights) + tf.matmul(\n",
    "            input_batch, curr_weights)\n",
    "        return output\n",
    "\n",
    "    def _generator_causal_layer(self, input_batch, state_batch):\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            output = self._generator_conv(\n",
    "                input_batch, state_batch, weights_filter)\n",
    "        return output\n",
    "\n",
    "    def _generator_dilation_layer(self, input_batch, state_batch, layer_index,\n",
    "                                  dilation):\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "        output_filter = self._generator_conv(\n",
    "            input_batch, state_batch, weights_filter)\n",
    "        output_gate = self._generator_conv(\n",
    "            input_batch, state_batch, weights_gate)\n",
    "\n",
    "        if self.use_biases:\n",
    "            output_filter = output_filter + variables['filter_bias']\n",
    "            output_gate = output_gate + variables['gate_bias']\n",
    "\n",
    "        out = tf.tanh(output_filter) * tf.sigmoid(output_gate)\n",
    "\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.matmul(out, weights_dense[0, :, :])\n",
    "        if self.use_biases:\n",
    "            transformed = transformed + variables['dense_bias']\n",
    "\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.matmul(out, weights_skip[0, :, :])\n",
    "        if self.use_biases:\n",
    "            skip_contribution = skip_contribution + variables['skip_bias']\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _create_network(self, input_batch):\n",
    "        '''Construct the WaveNet network.'''\n",
    "        outputs = []\n",
    "        current_layer = input_batch\n",
    "\n",
    "        # Pre-process the input with a regular convolution\n",
    "        if self.scalar_input:\n",
    "            initial_channels = 1\n",
    "        else:\n",
    "            initial_channels = self.quantization_channels\n",
    "\n",
    "        current_layer = self._create_causal_layer(current_layer)\n",
    "\n",
    "        output_width = tf.shape(input_batch)[1] - self.receptive_field + 1\n",
    "\n",
    "        # Add all defined dilation layers.\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                    output, current_layer = self._create_dilation_layer(\n",
    "                        current_layer, layer_index, dilation,\n",
    "                        output_width)\n",
    "                    outputs.append(output)\n",
    "\n",
    "        with tf.name_scope('postprocessing'):\n",
    "            # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "            # postprocess the output.\n",
    "            w1 = self.variables['postprocessing']['postprocess1']\n",
    "            w2 = self.variables['postprocessing']['postprocess2']\n",
    "            if self.use_biases:\n",
    "                b1 = self.variables['postprocessing']['postprocess1_bias']\n",
    "                b2 = self.variables['postprocessing']['postprocess2_bias']\n",
    "\n",
    "            if self.histograms:\n",
    "                tf.summary.histogram('postprocess1_weights', w1)\n",
    "                tf.summary.histogram('postprocess2_weights', w2)\n",
    "                if self.use_biases:\n",
    "                    tf.summary.histogram('postprocess1_biases', b1)\n",
    "                    tf.summary.histogram('postprocess2_biases', b2)\n",
    "\n",
    "            # We skip connections from the outputs of each layer, adding them\n",
    "            # all up here.\n",
    "            total = sum(outputs)\n",
    "            transformed1 = tf.nn.relu(total)\n",
    "            conv1 = tf.nn.conv1d(transformed1, w1, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv1 = tf.add(conv1, b1)\n",
    "            transformed2 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.nn.conv1d(transformed2, w2, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv2 = tf.add(conv2, b2)\n",
    "\n",
    "            ## ADDED CODE HERE. ******************************\n",
    "            ## RELU activation before going to dense node\n",
    "            transformed3 = tf.nn.relu(conv2)\n",
    "\n",
    "            ## Reshape to 2-D tensor with dimensions\n",
    "            ## batch_size, sample_size\n",
    "            ## shape[1] constant here so we can use dense\n",
    "            shape = [tf.shape(transformed3)[0], SAMPLE_SIZE - sum(dilations) - 1]\n",
    "            transformed3 = tf.reshape(transformed3, shape)\n",
    "            \n",
    "            ## Add dense layer to transform from\n",
    "            ## batch_size x sample_size 2-D array to\n",
    "            ## batch_size 1-D array\n",
    "            final = tf.layers.dense(transformed3, units=1, reuse=tf.AUTO_REUSE,\n",
    "                                    name='final_out')\n",
    "        return final\n",
    "\n",
    "    def _one_hot(self, input_batch):\n",
    "        '''One-hot encodes the waveform amplitudes.\n",
    "\n",
    "        This allows the definition of the network as a categorical distribution\n",
    "        over a finite set of possible amplitudes.\n",
    "        '''\n",
    "        with tf.name_scope('one_hot_encode'):\n",
    "            encoded = tf.one_hot(\n",
    "                input_batch,\n",
    "                depth=self.quantization_channels,\n",
    "                dtype=tf.float32)\n",
    "            shape = [self.batch_size, -1, self.quantization_channels]\n",
    "            encoded = tf.reshape(encoded, shape)\n",
    "        return encoded\n",
    "\n",
    "    def predict_proba(self, waveform, name='wavenet'):\n",
    "        '''Computes the probability of waveform being LAUGHTER.\n",
    "           Necessary for evaluation with test data.'''\n",
    "        ## TODO\n",
    "        ## tf.losses.sigmoid_cross_entropy(y_, train_out)\n",
    "        with tf.name_scope(name):\n",
    "            ## slice off rightmost end of input where labels are stored\n",
    "            ## input_batch transformed into input_audio\n",
    "            input_audio = tf.slice(waveform, begin=[0, 0, 0], size=[-1, SAMPLE_SIZE, -1])\n",
    "            input_label = tf.slice(waveform, begin=[0, SAMPLE_SIZE, 0], size=[-1, 1, -1])\n",
    "            \n",
    "            input_audio = tf.to_int32(input_audio)\n",
    "\n",
    "\n",
    "        with tf.name_scope(name):\n",
    "            if self.scalar_input:\n",
    "                encoded = tf.cast(waveform, tf.float32)\n",
    "                encoded = tf.reshape(encoded, [-1, 1])\n",
    "            else:\n",
    "                encoded = self._one_hot(input_audio)  # arg changed from waveform\n",
    "\n",
    "            raw_output = self._create_network(encoded)\n",
    "            raw_output = tf.reshape(raw_output, [-1])\n",
    "            out = tf.nn.sigmoid(raw_output)\n",
    "\n",
    "            return out\n",
    "\n",
    "        \n",
    "    def loss(self,\n",
    "             input_batch,\n",
    "             name='wavenet'):\n",
    "        '''Creates a WaveNet network and returns crossentropy loss.\n",
    "\n",
    "        The variables are all scoped to the given name.\n",
    "        '''\n",
    "## tf.losses.sigmoid_cross_entropy(y_, train_out)\n",
    "        with tf.name_scope(name):\n",
    "            ## slice off rightmost end of input where labels are stored\n",
    "            ## input_batch transformed into input_audio\n",
    "            y_idx = tf.shape(input_batch)[1] - 1\n",
    "            input_audio = tf.slice(input_batch, begin=[0, 0, 0], size=[-1, SAMPLE_SIZE, -1])\n",
    "            input_label = tf.slice(input_batch, begin=[0, SAMPLE_SIZE, 0], size=[-1, 1, -1])\n",
    "            \n",
    "            ## mu-law encode disabled because using preprocessed data\n",
    "            # encoded_input = mu_law_encode(input_batch,\n",
    "            #                             self.quantization_channels)\n",
    "            input_audio = tf.to_int32(input_audio)\n",
    "            encoded = self._one_hot(input_audio)  # arg changed from encoded_input\n",
    "            if self.scalar_input:\n",
    "                network_input = tf.reshape(\n",
    "                    tf.cast(input_batch, tf.float32),\n",
    "                    [self.batch_size, -1, 1])\n",
    "            else:\n",
    "                network_input = encoded\n",
    "\n",
    "              ## omitted for classification use\n",
    "#             # Cut off the last sample of network input to preserve causality.\n",
    "#             network_input_width = tf.shape(network_input)[1] - 1\n",
    "#             network_input = tf.slice(network_input, [0, 0, 0],\n",
    "#                                      [-1, network_input_width, -1])\n",
    "\n",
    "            ## shape of raw_output is (batch_size, 1)\n",
    "            raw_output = self._create_network(network_input)\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "                ## format the target values\n",
    "                target_output = tf.reshape(input_label, [self.batch_size])\n",
    "                ## logits\n",
    "                prediction = tf.reshape(raw_output, [self.batch_size])\n",
    "             \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=prediction,\n",
    "                    labels=target_output)\n",
    "                reduced_loss = tf.reduce_mean(loss)\n",
    "\n",
    "                tf.summary.scalar('loss', reduced_loss)\n",
    "\n",
    "                return reduced_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 40\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "net = WaveNetModel(batch_size=batch_size, dilations=dilations, filter_width=filter_width,\n",
    "                   residual_channels=residual_channels, dilation_channels=dilation_channels,\n",
    "                   skip_channels=skip_channels,\n",
    "                   histograms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df.sample(1089).copy()\n",
    "df1 = df0.iloc[:800, :]\n",
    "df_test = df0.iloc[800:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch1 = df1.sample(batch_size).iloc[:, 2:3603].values\n",
    "batch1 = batch1.reshape(batch_size, -1, 1)\n",
    "\n",
    "batch1_tensor = tf.Variable(batch1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss = net.loss(batch1_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'wavenet_1/loss/Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=1e-3,\n",
    "                                       momentum=0.9)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./tf_logs/train/{}'\n",
    "                        .format(datetime.now().strftime(\"%y%m%d_%H%M%S\")))\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "run_metadata = tf.RunMetadata()\n",
    "summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up session\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# saver code not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   1: loss =   0.703698\n",
      "step   2: loss =   0.701487\n",
      "step   3: loss =   0.697424\n",
      "step   4: loss =   0.691931\n",
      "step   5: loss =   0.685447\n",
      "CPU times: user 1min 31s, sys: 2.91 s, total: 1min 34s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for step in range(5):\n",
    "    summ_, loss_, _ = sess.run([summaries, loss, optim])\n",
    "    writer.add_summary(summ_, step)\n",
    "    print(\"step {:3}: loss = {:10.6f}\".format(step+1, loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test = df_test.sample(1).iloc[:, 2:3603].values\n",
    "batch_test = batch_test.reshape(1, -1, 1)\n",
    "\n",
    "batch_test_tensor = tf.Variable(batch_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sound_id</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>3591</th>\n",
       "      <th>3592</th>\n",
       "      <th>3593</th>\n",
       "      <th>3594</th>\n",
       "      <th>3595</th>\n",
       "      <th>3596</th>\n",
       "      <th>3597</th>\n",
       "      <th>3598</th>\n",
       "      <th>3599</th>\n",
       "      <th>laugh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>171947</td>\n",
       "      <td>7000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>...</td>\n",
       "      <td>176.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>93661</td>\n",
       "      <td>33000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>41015</td>\n",
       "      <td>30500</td>\n",
       "      <td>160.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>136.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>110162</td>\n",
       "      <td>79000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>175.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>262567</td>\n",
       "      <td>9500</td>\n",
       "      <td>160.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>216.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>41710</td>\n",
       "      <td>20000</td>\n",
       "      <td>160.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 3603 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sound_id  start_idx      0      1      2      3      4      5      6  \\\n",
       "79      171947       7000  160.0  150.0  138.0  124.0  112.0  106.0  107.0   \n",
       "1062     93661      33000  160.0  104.0   79.0   70.0   66.0   65.0   64.0   \n",
       "983      41015      30500  160.0  166.0  168.0  158.0  124.0   96.0   88.0   \n",
       "954     110162      79000  160.0  119.0  119.0  141.0   98.0  102.0  153.0   \n",
       "185     262567       9500  160.0   32.0   30.0   23.0   17.0   30.0   31.0   \n",
       "558      41710      20000  160.0  156.0  162.0  172.0  183.0  192.0  199.0   \n",
       "\n",
       "          7  ...     3591   3592   3593   3594   3595   3596   3597   3598  \\\n",
       "79    119.0  ...    176.0  175.0  176.0  178.0  179.0  178.0  173.0  166.0   \n",
       "1062   63.0  ...     46.0   45.0   46.0   48.0   49.0   50.0   52.0   54.0   \n",
       "983    88.0  ...    136.0  101.0   85.0   95.0  147.0  128.0   89.0   90.0   \n",
       "954   154.0  ...    175.0  181.0  179.0  171.0  159.0   99.0  103.0  161.0   \n",
       "185    49.0  ...    216.0  105.0  177.0  185.0   59.0   69.0   72.0   70.0   \n",
       "558   205.0  ...     66.0   64.0   63.0   64.0   67.0   72.0   81.0   98.0   \n",
       "\n",
       "       3599  laugh  \n",
       "79    156.0    0.0  \n",
       "1062   58.0    1.0  \n",
       "983   100.0    1.0  \n",
       "954   141.0    1.0  \n",
       "185    69.0    0.0  \n",
       "558   146.0    1.0  \n",
       "\n",
       "[6 rows x 3603 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['0'] == batch_test[0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 160.],\n",
       "        [ 104.],\n",
       "        [  79.],\n",
       "        ..., \n",
       "        [  54.],\n",
       "        [  58.],\n",
       "        [   1.]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=(1, 3601, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'wavenet_5/Sigmoid:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected begin[1] in [0, 88], but got 2045\n\t [[Node: wavenet_3/dilated_stack/layer0/Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wavenet_3/dilated_stack/layer0/mul, wavenet_3/dilated_stack/layer0/Slice/begin, wavenet_3/dilated_stack/layer0/Slice/size)]]\n\nCaused by op 'wavenet_3/dilated_stack/layer0/Slice', defined at:\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-162-e14b24d8d87e>\", line 1, in <module>\n    pred = net.predict_proba(batch_test_tensor)\n  File \"<ipython-input-112-d54c93656ae4>\", line 405, in predict_proba\n    raw_output = self._create_network(encoded)\n  File \"<ipython-input-112-d54c93656ae4>\", line 321, in _create_network\n    output_width)\n  File \"<ipython-input-112-d54c93656ae4>\", line 227, in _create_dilation_layer\n    out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 590, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4635, in _slice\n    \"Slice\", input=input, begin=begin, size=size, name=name)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Expected begin[1] in [0, 88], but got 2045\n\t [[Node: wavenet_3/dilated_stack/layer0/Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wavenet_3/dilated_stack/layer0/mul, wavenet_3/dilated_stack/layer0/Slice/begin, wavenet_3/dilated_stack/layer0/Slice/size)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected begin[1] in [0, 88], but got 2045\n\t [[Node: wavenet_3/dilated_stack/layer0/Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wavenet_3/dilated_stack/layer0/mul, wavenet_3/dilated_stack/layer0/Slice/begin, wavenet_3/dilated_stack/layer0/Slice/size)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-e14b24d8d87e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpred_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected begin[1] in [0, 88], but got 2045\n\t [[Node: wavenet_3/dilated_stack/layer0/Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wavenet_3/dilated_stack/layer0/mul, wavenet_3/dilated_stack/layer0/Slice/begin, wavenet_3/dilated_stack/layer0/Slice/size)]]\n\nCaused by op 'wavenet_3/dilated_stack/layer0/Slice', defined at:\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-162-e14b24d8d87e>\", line 1, in <module>\n    pred = net.predict_proba(batch_test_tensor)\n  File \"<ipython-input-112-d54c93656ae4>\", line 405, in predict_proba\n    raw_output = self._create_network(encoded)\n  File \"<ipython-input-112-d54c93656ae4>\", line 321, in _create_network\n    output_width)\n  File \"<ipython-input-112-d54c93656ae4>\", line 227, in _create_dilation_layer\n    out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 590, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4635, in _slice\n    \"Slice\", input=input, begin=begin, size=size, name=name)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/aeatda/anaconda3/envs/proj3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Expected begin[1] in [0, 88], but got 2045\n\t [[Node: wavenet_3/dilated_stack/layer0/Slice = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](wavenet_3/dilated_stack/layer0/mul, wavenet_3/dilated_stack/layer0/Slice/begin, wavenet_3/dilated_stack/layer0/Slice/size)]]\n"
     ]
    }
   ],
   "source": [
    "pred = net.predict_proba(batch_test_tensor)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "pred_ = sess.run(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_caching_device': None,\n",
       " '_constraint': None,\n",
       " '_initial_value': <tf.Tensor 'wavenet/causal_layer/random_uniform:0' shape=(2, 256, 16) dtype=float32>,\n",
       " '_initializer_op': <tf.Operation 'wavenet/causal_layer/filter/Assign' type=Assign>,\n",
       " '_save_slice_info': None,\n",
       " '_snapshot': <tf.Tensor 'wavenet/causal_layer/filter/read:0' shape=(2, 256, 16) dtype=float32>,\n",
       " '_variable': <tf.Tensor 'wavenet/causal_layer/filter:0' shape=(2, 256, 16) dtype=float32_ref>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(net.variables['causal_layer']['filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'causal_layer': {'filter': <tf.Variable 'wavenet/causal_layer/filter:0' shape=(2, 256, 16) dtype=float32_ref>},\n",
      " 'dilated_stack': [{'dense': <tf.Variable 'wavenet/dilated_stack/layer0/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer0/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer0/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer0/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer1/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer1/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer1/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer1/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer2/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer2/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer2/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer2/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer3/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer3/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer3/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer3/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer4/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer4/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer4/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer4/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer5/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer5/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer5/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer5/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer6/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer6/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer6/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer6/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer7/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer7/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer7/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer7/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer8/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer8/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer8/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer8/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer9/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer9/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer9/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer9/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer10/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer10/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer10/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer10/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer11/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer11/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer11/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer11/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer12/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer12/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer12/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer12/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer13/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer13/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer13/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer13/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer14/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer14/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer14/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer14/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer15/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer15/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer15/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer15/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer16/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer16/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer16/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer16/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer17/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer17/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer17/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer17/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer18/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer18/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer18/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer18/skip:0' shape=(1, 16, 16) dtype=float32_ref>},\n",
      "                   {'dense': <tf.Variable 'wavenet/dilated_stack/layer19/dense:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'filter': <tf.Variable 'wavenet/dilated_stack/layer19/filter:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'gate': <tf.Variable 'wavenet/dilated_stack/layer19/gate:0' shape=(2, 16, 16) dtype=float32_ref>,\n",
      "                    'skip': <tf.Variable 'wavenet/dilated_stack/layer19/skip:0' shape=(1, 16, 16) dtype=float32_ref>}],\n",
      " 'postprocessing': {'postprocess1': <tf.Variable 'wavenet/postprocessing/postprocess1:0' shape=(1, 16, 16) dtype=float32_ref>,\n",
      "                    'postprocess2': <tf.Variable 'wavenet/postprocessing/postprocess2:0' shape=(1, 16, 1) dtype=float32_ref>}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(net.variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proj3)",
   "language": "python",
   "name": "proj3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
