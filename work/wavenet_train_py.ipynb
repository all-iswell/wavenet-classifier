{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Training script for the WaveNet network on the VCTK corpus.\n",
    "\n",
    "This script trains a network with the WaveNet using data from the VCTK corpus,\n",
    "which can be freely downloaded at the following site (~10 GB):\n",
    "http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "from wavenet import WaveNetModel, AudioReader, optimizer_factory\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "DATA_DIRECTORY = './VCTK-Corpus'\n",
    "LOGDIR_ROOT = './logdir'\n",
    "CHECKPOINT_EVERY = 50\n",
    "NUM_STEPS = int(1e5)\n",
    "LEARNING_RATE = 1e-3\n",
    "WAVENET_PARAMS = './wavenet_params.json'\n",
    "STARTED_DATESTRING = \"{0:%Y-%m-%dT%H-%M-%S}\".format(datetime.now())\n",
    "SAMPLE_SIZE = 100000\n",
    "L2_REGULARIZATION_STRENGTH = 0\n",
    "SILENCE_THRESHOLD = 0.3\n",
    "EPSILON = 0.001\n",
    "MOMENTUM = 0.9\n",
    "MAX_TO_KEEP = 5\n",
    "METADATA = False\n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "    def _str_to_bool(s):\n",
    "        \"\"\"Convert string to bool (in argparse context).\"\"\"\n",
    "        if s.lower() not in ['true', 'false']:\n",
    "            raise ValueError('Argument needs to be a '\n",
    "                             'boolean, got {}'.format(s))\n",
    "        return {'true': True, 'false': False}[s.lower()]\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='WaveNet example network')\n",
    "    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE,\n",
    "                        help='How many wav files to process at once. Default: ' + str(BATCH_SIZE) + '.')\n",
    "    parser.add_argument('--data_dir', type=str, default=DATA_DIRECTORY,\n",
    "                        help='The directory containing the VCTK corpus.')\n",
    "    parser.add_argument('--store_metadata', type=bool, default=METADATA,\n",
    "                        help='Whether to store advanced debugging information '\n",
    "                        '(execution time, memory consumption) for use with '\n",
    "                        'TensorBoard. Default: ' + str(METADATA) + '.')\n",
    "    parser.add_argument('--logdir', type=str, default=None,\n",
    "                        help='Directory in which to store the logging '\n",
    "                        'information for TensorBoard. '\n",
    "                        'If the model already exists, it will restore '\n",
    "                        'the state and will continue training. '\n",
    "                        'Cannot use with --logdir_root and --restore_from.')\n",
    "    parser.add_argument('--logdir_root', type=str, default=None,\n",
    "                        help='Root directory to place the logging '\n",
    "                        'output and generated model. These are stored '\n",
    "                        'under the dated subdirectory of --logdir_root. '\n",
    "                        'Cannot use with --logdir.')\n",
    "    parser.add_argument('--restore_from', type=str, default=None,\n",
    "                        help='Directory in which to restore the model from. '\n",
    "                        'This creates the new model under the dated directory '\n",
    "                        'in --logdir_root. '\n",
    "                        'Cannot use with --logdir.')\n",
    "    parser.add_argument('--checkpoint_every', type=int,\n",
    "                        default=CHECKPOINT_EVERY,\n",
    "                        help='How many steps to save each checkpoint after. Default: ' + str(CHECKPOINT_EVERY) + '.')\n",
    "    parser.add_argument('--num_steps', type=int, default=NUM_STEPS,\n",
    "                        help='Number of training steps. Default: ' + str(NUM_STEPS) + '.')\n",
    "    parser.add_argument('--learning_rate', type=float, default=LEARNING_RATE,\n",
    "                        help='Learning rate for training. Default: ' + str(LEARNING_RATE) + '.')\n",
    "    parser.add_argument('--wavenet_params', type=str, default=WAVENET_PARAMS,\n",
    "                        help='JSON file with the network parameters. Default: ' + WAVENET_PARAMS + '.')\n",
    "    parser.add_argument('--sample_size', type=int, default=SAMPLE_SIZE,\n",
    "                        help='Concatenate and cut audio samples to this many '\n",
    "                        'samples. Default: ' + str(SAMPLE_SIZE) + '.')\n",
    "    parser.add_argument('--l2_regularization_strength', type=float,\n",
    "                        default=L2_REGULARIZATION_STRENGTH,\n",
    "                        help='Coefficient in the L2 regularization. '\n",
    "                        'Default: False')\n",
    "    parser.add_argument('--silence_threshold', type=float,\n",
    "                        default=SILENCE_THRESHOLD,\n",
    "                        help='Volume threshold below which to trim the start '\n",
    "                        'and the end from the training set samples. Default: ' + str(SILENCE_THRESHOLD) + '.')\n",
    "    parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                        choices=optimizer_factory.keys(),\n",
    "                        help='Select the optimizer specified by this option. Default: adam.')\n",
    "    parser.add_argument('--momentum', type=float,\n",
    "                        default=MOMENTUM, help='Specify the momentum to be '\n",
    "                        'used by sgd or rmsprop optimizer. Ignored by the '\n",
    "                        'adam optimizer. Default: ' + str(MOMENTUM) + '.')\n",
    "    parser.add_argument('--histograms', type=_str_to_bool, default=False,\n",
    "                        help='Whether to store histogram summaries. Default: False')\n",
    "    parser.add_argument('--gc_channels', type=int, default=None,\n",
    "                        help='Number of global condition channels. Default: None. Expecting: Int')\n",
    "    parser.add_argument('--max_checkpoints', type=int, default=MAX_TO_KEEP,\n",
    "                        help='Maximum amount of checkpoints that will be kept alive. Default: '\n",
    "                             + str(MAX_TO_KEEP) + '.')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')\n",
    "\n",
    "\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_default_logdir(logdir_root):\n",
    "    logdir = os.path.join(logdir_root, 'train', STARTED_DATESTRING)\n",
    "    return logdir\n",
    "\n",
    "\n",
    "def validate_directories(args):\n",
    "    \"\"\"Validate and arrange directory related arguments.\"\"\"\n",
    "\n",
    "    # Validation\n",
    "    if args.logdir and args.logdir_root:\n",
    "        raise ValueError(\"--logdir and --logdir_root cannot be \"\n",
    "                         \"specified at the same time.\")\n",
    "\n",
    "    if args.logdir and args.restore_from:\n",
    "        raise ValueError(\n",
    "            \"--logdir and --restore_from cannot be specified at the same \"\n",
    "            \"time. This is to keep your previous model from unexpected \"\n",
    "            \"overwrites.\\n\"\n",
    "            \"Use --logdir_root to specify the root of the directory which \"\n",
    "            \"will be automatically created with current date and time, or use \"\n",
    "            \"only --logdir to just continue the training from the last \"\n",
    "            \"checkpoint.\")\n",
    "\n",
    "    # Arrangement\n",
    "    logdir_root = args.logdir_root\n",
    "    if logdir_root is None:\n",
    "        logdir_root = LOGDIR_ROOT\n",
    "\n",
    "    logdir = args.logdir\n",
    "    if logdir is None:\n",
    "        logdir = get_default_logdir(logdir_root)\n",
    "        print('Using default logdir: {}'.format(logdir))\n",
    "\n",
    "    restore_from = args.restore_from\n",
    "    if restore_from is None:\n",
    "        # args.logdir and args.restore_from are exclusive,\n",
    "        # so it is guaranteed the logdir here is newly created.\n",
    "        restore_from = logdir\n",
    "\n",
    "    return {\n",
    "        'logdir': logdir,\n",
    "        'logdir_root': args.logdir_root,\n",
    "        'restore_from': restore_from\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_arguments()\n",
    "\n",
    "    try:\n",
    "        directories = validate_directories(args)\n",
    "    except ValueError as e:\n",
    "        print(\"Some arguments are wrong:\")\n",
    "        print(str(e))\n",
    "        return\n",
    "\n",
    "    logdir = directories['logdir']\n",
    "    restore_from = directories['restore_from']\n",
    "\n",
    "    # Even if we restored the model, we will treat it as new training\n",
    "    # if the trained model is written into an arbitrary location.\n",
    "    is_overwritten_training = logdir != restore_from\n",
    "\n",
    "    with open(args.wavenet_params, 'r') as f:\n",
    "        wavenet_params = json.load(f)\n",
    "\n",
    "    # Create coordinator.\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # Load raw waveform from VCTK corpus.\n",
    "    with tf.name_scope('create_inputs'):\n",
    "        # Allow silence trimming to be skipped by specifying a threshold near\n",
    "        # zero.\n",
    "        silence_threshold = args.silence_threshold if args.silence_threshold > \\\n",
    "                                                      EPSILON else None\n",
    "        gc_enabled = args.gc_channels is not None\n",
    "        reader = AudioReader(\n",
    "            args.data_dir,\n",
    "            coord,\n",
    "            sample_rate=wavenet_params['sample_rate'],\n",
    "            gc_enabled=gc_enabled,\n",
    "            receptive_field=WaveNetModel.calculate_receptive_field(wavenet_params[\"filter_width\"],\n",
    "                                                                   wavenet_params[\"dilations\"],\n",
    "                                                                   wavenet_params[\"scalar_input\"],\n",
    "                                                                   wavenet_params[\"initial_filter_width\"]),\n",
    "            sample_size=args.sample_size,\n",
    "            silence_threshold=silence_threshold)\n",
    "        audio_batch = reader.dequeue(args.batch_size)\n",
    "        if gc_enabled:\n",
    "            gc_id_batch = reader.dequeue_gc(args.batch_size)\n",
    "        else:\n",
    "            gc_id_batch = None\n",
    "\n",
    "    # Create network.\n",
    "    net = WaveNetModel(\n",
    "        batch_size=args.batch_size,\n",
    "        dilations=wavenet_params[\"dilations\"],\n",
    "        filter_width=wavenet_params[\"filter_width\"],\n",
    "        residual_channels=wavenet_params[\"residual_channels\"],\n",
    "        dilation_channels=wavenet_params[\"dilation_channels\"],\n",
    "        skip_channels=wavenet_params[\"skip_channels\"],\n",
    "        quantization_channels=wavenet_params[\"quantization_channels\"],\n",
    "        use_biases=wavenet_params[\"use_biases\"],\n",
    "        scalar_input=wavenet_params[\"scalar_input\"],\n",
    "        initial_filter_width=wavenet_params[\"initial_filter_width\"],\n",
    "        histograms=args.histograms,\n",
    "        global_condition_channels=args.gc_channels,\n",
    "        global_condition_cardinality=reader.gc_category_cardinality)\n",
    "\n",
    "    if args.l2_regularization_strength == 0:\n",
    "        args.l2_regularization_strength = None\n",
    "    loss = net.loss(input_batch=audio_batch,\n",
    "                    global_condition_batch=gc_id_batch,\n",
    "                    l2_regularization_strength=args.l2_regularization_strength)\n",
    "    optimizer = optimizer_factory[args.optimizer](\n",
    "                    learning_rate=args.learning_rate,\n",
    "                    momentum=args.momentum)\n",
    "    trainable = tf.trainable_variables()\n",
    "    optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "    # Set up logging for TensorBoard.\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(tf.get_default_graph())\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summaries = tf.summary.merge_all()\n",
    "\n",
    "    # Set up session\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Saver for storing checkpoints of the model.\n",
    "    saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=args.max_checkpoints)\n",
    "\n",
    "    try:\n",
    "        saved_global_step = load(saver, sess, restore_from)\n",
    "        if is_overwritten_training or saved_global_step is None:\n",
    "            # The first training step will be saved_global_step + 1,\n",
    "            # therefore we put -1 here for new or overwritten trainings.\n",
    "            saved_global_step = -1\n",
    "\n",
    "    except:\n",
    "        print(\"Something went wrong while restoring checkpoint. \"\n",
    "              \"We will terminate training to avoid accidentally overwriting \"\n",
    "              \"the previous model.\")\n",
    "        raise\n",
    "\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    reader.start_threads(sess)\n",
    "\n",
    "    step = None\n",
    "    last_saved_step = saved_global_step\n",
    "    try:\n",
    "        for step in range(saved_global_step + 1, args.num_steps):\n",
    "            start_time = time.time()\n",
    "            if args.store_metadata and step % 50 == 0:\n",
    "                # Slow run that stores extra information for debugging.\n",
    "                print('Storing metadata')\n",
    "                run_options = tf.RunOptions(\n",
    "                    trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                summary, loss_value, _ = sess.run(\n",
    "                    [summaries, loss, optim],\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata)\n",
    "                writer.add_summary(summary, step)\n",
    "                writer.add_run_metadata(run_metadata,\n",
    "                                        'step_{:04d}'.format(step))\n",
    "                tl = timeline.Timeline(run_metadata.step_stats)\n",
    "                timeline_path = os.path.join(logdir, 'timeline.trace')\n",
    "                with open(timeline_path, 'w') as f:\n",
    "                    f.write(tl.generate_chrome_trace_format(show_memory=True))\n",
    "            else:\n",
    "                summary, loss_value, _ = sess.run([summaries, loss, optim])\n",
    "                writer.add_summary(summary, step)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "            print('step {:d} - loss = {:.3f}, ({:.3f} sec/step)'\n",
    "                  .format(step, loss_value, duration))\n",
    "\n",
    "            if step % args.checkpoint_every == 0:\n",
    "                save(saver, sess, logdir, step)\n",
    "                last_saved_step = step\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Introduce a line break after ^C is displayed so save message\n",
    "        # is on its own line.\n",
    "        print()\n",
    "    finally:\n",
    "        if step > last_saved_step:\n",
    "            save(saver, sess, logdir, step)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WaveNetModel(object):\n",
    "    '''Implements the WaveNet network for generative audio.\n",
    "\n",
    "    Usage (with the architecture as in the DeepMind paper):\n",
    "        dilations = [2**i for i in range(N)] * M\n",
    "        filter_width = 2  # Convolutions just use 2 samples.\n",
    "        residual_channels = 16  # Not specified in the paper.\n",
    "        dilation_channels = 32  # Not specified in the paper.\n",
    "        skip_channels = 16      # Not specified in the paper.\n",
    "        net = WaveNetModel(batch_size, dilations, filter_width,\n",
    "                           residual_channels, dilation_channels,\n",
    "                           skip_channels)\n",
    "        loss = net.loss(input_batch)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 dilations,\n",
    "                 filter_width,\n",
    "                 residual_channels,\n",
    "                 dilation_channels,\n",
    "                 skip_channels,\n",
    "                 quantization_channels=2**8,\n",
    "                 use_biases=False,\n",
    "                 scalar_input=False,\n",
    "                 initial_filter_width=32,\n",
    "                 histograms=False,\n",
    "                 global_condition_channels=None,\n",
    "                 global_condition_cardinality=None):\n",
    "        '''Initializes the WaveNet model.\n",
    "\n",
    "        Args:\n",
    "            batch_size: How many audio files are supplied per batch\n",
    "                (recommended: 1).\n",
    "            dilations: A list with the dilation factor for each layer.\n",
    "            filter_width: The samples that are included in each convolution,\n",
    "                after dilating.\n",
    "            residual_channels: How many filters to learn for the residual.\n",
    "            dilation_channels: How many filters to learn for the dilated\n",
    "                convolution.\n",
    "            skip_channels: How many filters to learn that contribute to the\n",
    "                quantized softmax output.\n",
    "            quantization_channels: How many amplitude values to use for audio\n",
    "                quantization and the corresponding one-hot encoding.\n",
    "                Default: 256 (8-bit quantization).\n",
    "            use_biases: Whether to add a bias layer to each convolution.\n",
    "                Default: False.\n",
    "            scalar_input: Whether to use the quantized waveform directly as\n",
    "                input to the network instead of one-hot encoding it.\n",
    "                Default: False.\n",
    "            initial_filter_width: The width of the initial filter of the\n",
    "                convolution applied to the scalar input. This is only relevant\n",
    "                if scalar_input=True.\n",
    "            histograms: Whether to store histograms in the summary.\n",
    "                Default: False.\n",
    "            global_condition_channels: Number of channels in (embedding\n",
    "                size) of global conditioning vector. None indicates there is\n",
    "                no global conditioning.\n",
    "            global_condition_cardinality: Number of mutually exclusive\n",
    "                categories to be embedded in global condition embedding. If\n",
    "                not None, then this implies that global_condition tensor\n",
    "                specifies an integer selecting which of the N global condition\n",
    "                categories, where N = global_condition_cardinality. If None,\n",
    "                then the global_condition tensor is regarded as a vector which\n",
    "                must have dimension global_condition_channels.\n",
    "\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.residual_channels = residual_channels\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.skip_channels = skip_channels\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.histograms = histograms\n",
    "        self.global_condition_channels = global_condition_channels\n",
    "        self.global_condition_cardinality = global_condition_cardinality\n",
    "\n",
    "        self.receptive_field = WaveNetModel.calculate_receptive_field(\n",
    "            self.filter_width, self.dilations, self.scalar_input,\n",
    "            self.initial_filter_width)\n",
    "        self.variables = self._create_variables()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_receptive_field(filter_width, dilations, scalar_input,\n",
    "                                  initial_filter_width):\n",
    "        receptive_field = (filter_width - 1) * sum(dilations) + 1\n",
    "        if scalar_input:\n",
    "            receptive_field += initial_filter_width - 1\n",
    "        else:\n",
    "            receptive_field += filter_width - 1\n",
    "        return receptive_field\n",
    "\n",
    "    def _create_variables(self):\n",
    "        '''This function creates all variables used by the network.\n",
    "        This allows us to share them between multiple calls to the loss\n",
    "        function and generation function.'''\n",
    "\n",
    "        var = dict()\n",
    "\n",
    "        with tf.variable_scope('wavenet'):\n",
    "            if self.global_condition_cardinality is not None:\n",
    "                # We only look up the embedding if we are conditioning on a\n",
    "                # set of mutually-exclusive categories. We can also condition\n",
    "                # on an already-embedded dense vector, in which case it's\n",
    "                # given to us and we don't need to do the embedding lookup.\n",
    "                # Still another alternative is no global condition at all, in\n",
    "                # which case we also don't do a tf.nn.embedding_lookup.\n",
    "                with tf.variable_scope('embeddings'):\n",
    "                    layer = dict()\n",
    "                    layer['gc_embedding'] = create_embedding_table(\n",
    "                        'gc_embedding',\n",
    "                        [self.global_condition_cardinality,\n",
    "                         self.global_condition_channels])\n",
    "                    var['embeddings'] = layer\n",
    "\n",
    "            with tf.variable_scope('causal_layer'):\n",
    "                layer = dict()\n",
    "                if self.scalar_input:\n",
    "                    initial_channels = 1\n",
    "                    initial_filter_width = self.initial_filter_width\n",
    "                else:\n",
    "                    initial_channels = self.quantization_channels\n",
    "                    initial_filter_width = self.filter_width\n",
    "                layer['filter'] = create_variable(\n",
    "                    'filter',\n",
    "                    [initial_filter_width,\n",
    "                     initial_channels,\n",
    "                     self.residual_channels])\n",
    "                var['causal_layer'] = layer\n",
    "\n",
    "            var['dilated_stack'] = list()\n",
    "            with tf.variable_scope('dilated_stack'):\n",
    "                for i, dilation in enumerate(self.dilations):\n",
    "                    with tf.variable_scope('layer{}'.format(i)):\n",
    "                        current = dict()\n",
    "                        current['filter'] = create_variable(\n",
    "                            'filter',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['gate'] = create_variable(\n",
    "                            'gate',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['dense'] = create_variable(\n",
    "                            'dense',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.residual_channels])\n",
    "                        current['skip'] = create_variable(\n",
    "                            'skip',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.skip_channels])\n",
    "\n",
    "                        if self.global_condition_channels is not None:\n",
    "                            current['gc_gateweights'] = create_variable(\n",
    "                                'gc_gate',\n",
    "                                [1, self.global_condition_channels,\n",
    "                                 self.dilation_channels])\n",
    "                            current['gc_filtweights'] = create_variable(\n",
    "                                'gc_filter',\n",
    "                                [1, self.global_condition_channels,\n",
    "                                 self.dilation_channels])\n",
    "\n",
    "                        if self.use_biases:\n",
    "                            current['filter_bias'] = create_bias_variable(\n",
    "                                'filter_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['gate_bias'] = create_bias_variable(\n",
    "                                'gate_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['dense_bias'] = create_bias_variable(\n",
    "                                'dense_bias',\n",
    "                                [self.residual_channels])\n",
    "                            current['skip_bias'] = create_bias_variable(\n",
    "                                'slip_bias',\n",
    "                                [self.skip_channels])\n",
    "\n",
    "                        var['dilated_stack'].append(current)\n",
    "\n",
    "            with tf.variable_scope('postprocessing'):\n",
    "                current = dict()\n",
    "                current['postprocess1'] = create_variable(\n",
    "                    'postprocess1',\n",
    "                    [1, self.skip_channels, self.skip_channels])\n",
    "                current['postprocess2'] = create_variable(\n",
    "                    'postprocess2',\n",
    "                    [1, self.skip_channels, self.quantization_channels])\n",
    "                if self.use_biases:\n",
    "                    current['postprocess1_bias'] = create_bias_variable(\n",
    "                        'postprocess1_bias',\n",
    "                        [self.skip_channels])\n",
    "                    current['postprocess2_bias'] = create_bias_variable(\n",
    "                        'postprocess2_bias',\n",
    "                        [self.quantization_channels])\n",
    "                var['postprocessing'] = current\n",
    "\n",
    "        return var\n",
    "\n",
    "    def _create_causal_layer(self, input_batch):\n",
    "        '''Creates a single causal convolution layer.\n",
    "\n",
    "        The layer can change the number of channels.\n",
    "        '''\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            return causal_conv(input_batch, weights_filter, 1)\n",
    "\n",
    "    def _create_dilation_layer(self, input_batch, layer_index, dilation,\n",
    "                               global_condition_batch, output_width):\n",
    "        '''Creates a single causal dilated convolution layer.\n",
    "\n",
    "        Args:\n",
    "             input_batch: Input to the dilation layer.\n",
    "             layer_index: Integer indicating which layer this is.\n",
    "             dilation: Integer specifying the dilation size.\n",
    "             global_conditioning_batch: Tensor containing the global data upon\n",
    "                 which the output is to be conditioned upon. Shape:\n",
    "                 [batch size, 1, channels]. The 1 is for the axis\n",
    "                 corresponding to time so that the result is broadcast to\n",
    "                 all time steps.\n",
    "\n",
    "        The layer contains a gated filter that connects to dense output\n",
    "        and to a skip connection:\n",
    "\n",
    "               |-> [gate]   -|        |-> 1x1 conv -> skip output\n",
    "               |             |-> (*) -|\n",
    "        input -|-> [filter] -|        |-> 1x1 conv -|\n",
    "               |                                    |-> (+) -> dense output\n",
    "               |------------------------------------|\n",
    "\n",
    "        Where `[gate]` and `[filter]` are causal convolutions with a\n",
    "        non-linear activation at the output. Biases and global conditioning\n",
    "        are omitted due to the limits of ASCII art.\n",
    "\n",
    "        '''\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "\n",
    "        conv_filter = causal_conv(input_batch, weights_filter, dilation)\n",
    "        conv_gate = causal_conv(input_batch, weights_gate, dilation)\n",
    "\n",
    "        if global_condition_batch is not None:\n",
    "            weights_gc_filter = variables['gc_filtweights']\n",
    "            conv_filter = conv_filter + tf.nn.conv1d(global_condition_batch,\n",
    "                                                     weights_gc_filter,\n",
    "                                                     stride=1,\n",
    "                                                     padding=\"SAME\",\n",
    "                                                     name=\"gc_filter\")\n",
    "            weights_gc_gate = variables['gc_gateweights']\n",
    "            conv_gate = conv_gate + tf.nn.conv1d(global_condition_batch,\n",
    "                                                 weights_gc_gate,\n",
    "                                                 stride=1,\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 name=\"gc_gate\")\n",
    "\n",
    "        if self.use_biases:\n",
    "            filter_bias = variables['filter_bias']\n",
    "            gate_bias = variables['gate_bias']\n",
    "            conv_filter = tf.add(conv_filter, filter_bias)\n",
    "            conv_gate = tf.add(conv_gate, gate_bias)\n",
    "\n",
    "        out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\n",
    "\n",
    "        # The 1x1 conv to produce the residual output\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.nn.conv1d(\n",
    "            out, weights_dense, stride=1, padding=\"SAME\", name=\"dense\")\n",
    "\n",
    "        # The 1x1 conv to produce the skip output\n",
    "        skip_cut = tf.shape(out)[1] - output_width\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.nn.conv1d(\n",
    "            out_skip, weights_skip, stride=1, padding=\"SAME\", name=\"skip\")\n",
    "\n",
    "        if self.use_biases:\n",
    "            dense_bias = variables['dense_bias']\n",
    "            skip_bias = variables['skip_bias']\n",
    "            transformed = transformed + dense_bias\n",
    "            skip_contribution = skip_contribution + skip_bias\n",
    "\n",
    "        if self.histograms:\n",
    "            layer = 'layer{}'.format(layer_index)\n",
    "            tf.histogram_summary(layer + '_filter', weights_filter)\n",
    "            tf.histogram_summary(layer + '_gate', weights_gate)\n",
    "            tf.histogram_summary(layer + '_dense', weights_dense)\n",
    "            tf.histogram_summary(layer + '_skip', weights_skip)\n",
    "            if self.use_biases:\n",
    "                tf.histogram_summary(layer + '_biases_filter', filter_bias)\n",
    "                tf.histogram_summary(layer + '_biases_gate', gate_bias)\n",
    "                tf.histogram_summary(layer + '_biases_dense', dense_bias)\n",
    "                tf.histogram_summary(layer + '_biases_skip', skip_bias)\n",
    "\n",
    "        input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\n",
    "        input_batch = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _generator_conv(self, input_batch, state_batch, weights):\n",
    "        '''Perform convolution for a single convolutional processing step.'''\n",
    "        # TODO generalize to filter_width > 2\n",
    "        past_weights = weights[0, :, :]\n",
    "        curr_weights = weights[1, :, :]\n",
    "        output = tf.matmul(state_batch, past_weights) + tf.matmul(\n",
    "            input_batch, curr_weights)\n",
    "        return output\n",
    "\n",
    "    def _generator_causal_layer(self, input_batch, state_batch):\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            output = self._generator_conv(\n",
    "                input_batch, state_batch, weights_filter)\n",
    "        return output\n",
    "\n",
    "    def _generator_dilation_layer(self, input_batch, state_batch, layer_index,\n",
    "                                  dilation, global_condition_batch):\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "        output_filter = self._generator_conv(\n",
    "            input_batch, state_batch, weights_filter)\n",
    "        output_gate = self._generator_conv(\n",
    "            input_batch, state_batch, weights_gate)\n",
    "\n",
    "        if global_condition_batch is not None:\n",
    "            global_condition_batch = tf.reshape(global_condition_batch,\n",
    "                                                shape=(1, -1))\n",
    "            weights_gc_filter = variables['gc_filtweights']\n",
    "            weights_gc_filter = weights_gc_filter[0, :, :]\n",
    "            output_filter += tf.matmul(global_condition_batch,\n",
    "                                       weights_gc_filter)\n",
    "            weights_gc_gate = variables['gc_gateweights']\n",
    "            weights_gc_gate = weights_gc_gate[0, :, :]\n",
    "            output_gate += tf.matmul(global_condition_batch,\n",
    "                                     weights_gc_gate)\n",
    "\n",
    "        if self.use_biases:\n",
    "            output_filter = output_filter + variables['filter_bias']\n",
    "            output_gate = output_gate + variables['gate_bias']\n",
    "\n",
    "        out = tf.tanh(output_filter) * tf.sigmoid(output_gate)\n",
    "\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.matmul(out, weights_dense[0, :, :])\n",
    "        if self.use_biases:\n",
    "            transformed = transformed + variables['dense_bias']\n",
    "\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.matmul(out, weights_skip[0, :, :])\n",
    "        if self.use_biases:\n",
    "            skip_contribution = skip_contribution + variables['skip_bias']\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _create_network(self, input_batch, global_condition_batch):\n",
    "        '''Construct the WaveNet network.'''\n",
    "        outputs = []\n",
    "        current_layer = input_batch\n",
    "\n",
    "        # Pre-process the input with a regular convolution\n",
    "        if self.scalar_input:\n",
    "            initial_channels = 1\n",
    "        else:\n",
    "            initial_channels = self.quantization_channels\n",
    "\n",
    "        current_layer = self._create_causal_layer(current_layer)\n",
    "\n",
    "        output_width = tf.shape(input_batch)[1] - self.receptive_field + 1\n",
    "\n",
    "        # Add all defined dilation layers.\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                    output, current_layer = self._create_dilation_layer(\n",
    "                        current_layer, layer_index, dilation,\n",
    "                        global_condition_batch, output_width)\n",
    "                    outputs.append(output)\n",
    "\n",
    "        with tf.name_scope('postprocessing'):\n",
    "            # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "            # postprocess the output.\n",
    "            w1 = self.variables['postprocessing']['postprocess1']\n",
    "            w2 = self.variables['postprocessing']['postprocess2']\n",
    "            if self.use_biases:\n",
    "                b1 = self.variables['postprocessing']['postprocess1_bias']\n",
    "                b2 = self.variables['postprocessing']['postprocess2_bias']\n",
    "\n",
    "            if self.histograms:\n",
    "                tf.histogram_summary('postprocess1_weights', w1)\n",
    "                tf.histogram_summary('postprocess2_weights', w2)\n",
    "                if self.use_biases:\n",
    "                    tf.histogram_summary('postprocess1_biases', b1)\n",
    "                    tf.histogram_summary('postprocess2_biases', b2)\n",
    "\n",
    "            # We skip connections from the outputs of each layer, adding them\n",
    "            # all up here.\n",
    "            total = sum(outputs)\n",
    "            transformed1 = tf.nn.relu(total)\n",
    "            conv1 = tf.nn.conv1d(transformed1, w1, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv1 = tf.add(conv1, b1)\n",
    "            transformed2 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.nn.conv1d(transformed2, w2, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv2 = tf.add(conv2, b2)\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    def _create_generator(self, input_batch, global_condition_batch):\n",
    "        '''Construct an efficient incremental generator.'''\n",
    "        init_ops = []\n",
    "        push_ops = []\n",
    "        outputs = []\n",
    "        current_layer = input_batch\n",
    "\n",
    "        q = tf.FIFOQueue(\n",
    "            1,\n",
    "            dtypes=tf.float32,\n",
    "            shapes=(self.batch_size, self.quantization_channels))\n",
    "        init = q.enqueue_many(\n",
    "            tf.zeros((1, self.batch_size, self.quantization_channels)))\n",
    "\n",
    "        current_state = q.dequeue()\n",
    "        push = q.enqueue([current_layer])\n",
    "        init_ops.append(init)\n",
    "        push_ops.append(push)\n",
    "\n",
    "        current_layer = self._generator_causal_layer(\n",
    "                            current_layer, current_state)\n",
    "\n",
    "        # Add all defined dilation layers.\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "\n",
    "                    q = tf.FIFOQueue(\n",
    "                        dilation,\n",
    "                        dtypes=tf.float32,\n",
    "                        shapes=(self.batch_size, self.residual_channels))\n",
    "                    init = q.enqueue_many(\n",
    "                        tf.zeros((dilation, self.batch_size,\n",
    "                                  self.residual_channels)))\n",
    "\n",
    "                    current_state = q.dequeue()\n",
    "                    push = q.enqueue([current_layer])\n",
    "                    init_ops.append(init)\n",
    "                    push_ops.append(push)\n",
    "\n",
    "                    output, current_layer = self._generator_dilation_layer(\n",
    "                        current_layer, current_state, layer_index, dilation,\n",
    "                        global_condition_batch)\n",
    "                    outputs.append(output)\n",
    "        self.init_ops = init_ops\n",
    "        self.push_ops = push_ops\n",
    "\n",
    "        with tf.name_scope('postprocessing'):\n",
    "            variables = self.variables['postprocessing']\n",
    "            # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "            # postprocess the output.\n",
    "            w1 = variables['postprocess1']\n",
    "            w2 = variables['postprocess2']\n",
    "            if self.use_biases:\n",
    "                b1 = variables['postprocess1_bias']\n",
    "                b2 = variables['postprocess2_bias']\n",
    "\n",
    "            # We skip connections from the outputs of each layer, adding them\n",
    "            # all up here.\n",
    "            total = sum(outputs)\n",
    "            transformed1 = tf.nn.relu(total)\n",
    "\n",
    "            conv1 = tf.matmul(transformed1, w1[0, :, :])\n",
    "            if self.use_biases:\n",
    "                conv1 = conv1 + b1\n",
    "            transformed2 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.matmul(transformed2, w2[0, :, :])\n",
    "            if self.use_biases:\n",
    "                conv2 = conv2 + b2\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    def _one_hot(self, input_batch):\n",
    "        '''One-hot encodes the waveform amplitudes.\n",
    "\n",
    "        This allows the definition of the network as a categorical distribution\n",
    "        over a finite set of possible amplitudes.\n",
    "        '''\n",
    "        with tf.name_scope('one_hot_encode'):\n",
    "            encoded = tf.one_hot(\n",
    "                input_batch,\n",
    "                depth=self.quantization_channels,\n",
    "                dtype=tf.float32)\n",
    "            shape = [self.batch_size, -1, self.quantization_channels]\n",
    "            encoded = tf.reshape(encoded, shape)\n",
    "        return encoded\n",
    "\n",
    "    def _embed_gc(self, global_condition):\n",
    "        '''Returns embedding for global condition.\n",
    "        :param global_condition: Either ID of global condition for\n",
    "               tf.nn.embedding_lookup or actual embedding. The latter is\n",
    "               experimental.\n",
    "        :return: Embedding or None\n",
    "        '''\n",
    "        embedding = None\n",
    "        if self.global_condition_cardinality is not None:\n",
    "            # Only lookup the embedding if the global condition is presented\n",
    "            # as an integer of mutually-exclusive categories ...\n",
    "            embedding_table = self.variables['embeddings']['gc_embedding']\n",
    "            embedding = tf.nn.embedding_lookup(embedding_table,\n",
    "                                               global_condition)\n",
    "        elif global_condition is not None:\n",
    "            # ... else the global_condition (if any) is already provided\n",
    "            # as an embedding.\n",
    "\n",
    "            # In this case, the number of global_embedding channels must be\n",
    "            # equal to the the last dimension of the global_condition tensor.\n",
    "            gc_batch_rank = len(global_condition.get_shape())\n",
    "            dims_match = (global_condition.get_shape()[gc_batch_rank - 1] ==\n",
    "                          self.global_condition_channels)\n",
    "            if not dims_match:\n",
    "                raise ValueError('Shape of global_condition {} does not'\n",
    "                                 ' match global_condition_channels {}.'.\n",
    "                                 format(global_condition.get_shape(),\n",
    "                                        self.global_condition_channels))\n",
    "            embedding = global_condition\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding = tf.reshape(\n",
    "                embedding,\n",
    "                [self.batch_size, 1, self.global_condition_channels])\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    def predict_proba(self, waveform, global_condition=None, name='wavenet'):\n",
    "        '''Computes the probability distribution of the next sample based on\n",
    "        all samples in the input waveform.\n",
    "        If you want to generate audio by feeding the output of the network back\n",
    "        as an input, see predict_proba_incremental for a faster alternative.'''\n",
    "        with tf.name_scope(name):\n",
    "            if self.scalar_input:\n",
    "                encoded = tf.cast(waveform, tf.float32)\n",
    "                encoded = tf.reshape(encoded, [-1, 1])\n",
    "            else:\n",
    "                encoded = self._one_hot(waveform)\n",
    "\n",
    "            gc_embedding = self._embed_gc(global_condition)\n",
    "            raw_output = self._create_network(encoded, gc_embedding)\n",
    "            out = tf.reshape(raw_output, [-1, self.quantization_channels])\n",
    "            # Cast to float64 to avoid bug in TensorFlow\n",
    "            proba = tf.cast(\n",
    "                tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n",
    "            last = tf.slice(\n",
    "                proba,\n",
    "                [tf.shape(proba)[0] - 1, 0],\n",
    "                [1, self.quantization_channels])\n",
    "            return tf.reshape(last, [-1])\n",
    "\n",
    "    def predict_proba_incremental(self, waveform, global_condition=None,\n",
    "                                  name='wavenet'):\n",
    "        '''Computes the probability distribution of the next sample\n",
    "        incrementally, based on a single sample and all previously passed\n",
    "        samples.'''\n",
    "        if self.filter_width > 2:\n",
    "            raise NotImplementedError(\"Incremental generation does not \"\n",
    "                                      \"support filter_width > 2.\")\n",
    "        if self.scalar_input:\n",
    "            raise NotImplementedError(\"Incremental generation does not \"\n",
    "                                      \"support scalar input yet.\")\n",
    "        with tf.name_scope(name):\n",
    "            encoded = tf.one_hot(waveform, self.quantization_channels)\n",
    "            encoded = tf.reshape(encoded, [-1, self.quantization_channels])\n",
    "            gc_embedding = self._embed_gc(global_condition)\n",
    "            raw_output = self._create_generator(encoded, gc_embedding)\n",
    "            out = tf.reshape(raw_output, [-1, self.quantization_channels])\n",
    "            proba = tf.cast(\n",
    "                tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n",
    "            last = tf.slice(\n",
    "                proba,\n",
    "                [tf.shape(proba)[0] - 1, 0],\n",
    "                [1, self.quantization_channels])\n",
    "            return tf.reshape(last, [-1])\n",
    "\n",
    "    def loss(self,\n",
    "             input_batch,\n",
    "             global_condition_batch=None,\n",
    "             l2_regularization_strength=None,\n",
    "             name='wavenet'):\n",
    "        '''Creates a WaveNet network and returns the autoencoding loss.\n",
    "\n",
    "        The variables are all scoped to the given name.\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            # We mu-law encode and quantize the input audioform.\n",
    "            encoded_input = mu_law_encode(input_batch,\n",
    "                                          self.quantization_channels)\n",
    "\n",
    "            gc_embedding = self._embed_gc(global_condition_batch)\n",
    "            encoded = self._one_hot(encoded_input)\n",
    "            if self.scalar_input:\n",
    "                network_input = tf.reshape(\n",
    "                    tf.cast(input_batch, tf.float32),\n",
    "                    [self.batch_size, -1, 1])\n",
    "            else:\n",
    "                network_input = encoded\n",
    "\n",
    "            # Cut off the last sample of network input to preserve causality.\n",
    "            network_input_width = tf.shape(network_input)[1] - 1\n",
    "            network_input = tf.slice(network_input, [0, 0, 0],\n",
    "                                     [-1, network_input_width, -1])\n",
    "\n",
    "            raw_output = self._create_network(network_input, gc_embedding)\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "                # Cut off the samples corresponding to the receptive field\n",
    "                # for the first predicted sample.\n",
    "                target_output = tf.slice(\n",
    "                    tf.reshape(\n",
    "                        encoded,\n",
    "                        [self.batch_size, -1, self.quantization_channels]),\n",
    "                    [0, self.receptive_field, 0],\n",
    "                    [-1, -1, -1])\n",
    "                target_output = tf.reshape(target_output,\n",
    "                                           [-1, self.quantization_channels])\n",
    "                prediction = tf.reshape(raw_output,\n",
    "                                        [-1, self.quantization_channels])\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=prediction,\n",
    "                    labels=target_output)\n",
    "                reduced_loss = tf.reduce_mean(loss)\n",
    "\n",
    "                tf.summary.scalar('loss', reduced_loss)\n",
    "\n",
    "                if l2_regularization_strength is None:\n",
    "                    return reduced_loss\n",
    "                else:\n",
    "                    # L2 regularization for all trainable parameters\n",
    "                    l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                        for v in tf.trainable_variables()\n",
    "                                        if not('bias' in v.name)])\n",
    "\n",
    "                    # Add the regularization term to the loss\n",
    "                    total_loss = (reduced_loss +\n",
    "                                  l2_regularization_strength * l2_loss)\n",
    "\n",
    "                    tf.summary.scalar('l2_loss', l2_loss)\n",
    "                    tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "                    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AudioReader(object):\n",
    "    '''Generic background audio reader that preprocesses audio files\n",
    "    and enqueues them into a TensorFlow queue.'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 audio_dir,\n",
    "                 coord,\n",
    "                 sample_rate,\n",
    "                 gc_enabled,\n",
    "                 receptive_field,\n",
    "                 sample_size=None,\n",
    "                 silence_threshold=None,\n",
    "                 queue_size=32):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.coord = coord\n",
    "        self.sample_size = sample_size\n",
    "        self.receptive_field = receptive_field\n",
    "        self.silence_threshold = silence_threshold\n",
    "        self.gc_enabled = gc_enabled\n",
    "        self.threads = []\n",
    "        self.sample_placeholder = tf.placeholder(dtype=tf.float32, shape=None)\n",
    "        self.queue = tf.PaddingFIFOQueue(queue_size,\n",
    "                                         ['float32'],\n",
    "                                         shapes=[(None, 1)])\n",
    "        self.enqueue = self.queue.enqueue([self.sample_placeholder])\n",
    "\n",
    "        if self.gc_enabled:\n",
    "            self.id_placeholder = tf.placeholder(dtype=tf.int32, shape=())\n",
    "            self.gc_queue = tf.PaddingFIFOQueue(queue_size, ['int32'],\n",
    "                                                shapes=[()])\n",
    "            self.gc_enqueue = self.gc_queue.enqueue([self.id_placeholder])\n",
    "\n",
    "        # TODO Find a better way to check this.\n",
    "        # Checking inside the AudioReader's thread makes it hard to terminate\n",
    "        # the execution of the script, so we do it in the constructor for now.\n",
    "        files = find_files(audio_dir)\n",
    "        if not files:\n",
    "            raise ValueError(\"No audio files found in '{}'.\".format(audio_dir))\n",
    "        if self.gc_enabled and not_all_have_id(files):\n",
    "            raise ValueError(\"Global conditioning is enabled, but file names \"\n",
    "                             \"do not conform to pattern having id.\")\n",
    "        # Determine the number of mutually-exclusive categories we will\n",
    "        # accomodate in our embedding table.\n",
    "        if self.gc_enabled:\n",
    "            _, self.gc_category_cardinality = get_category_cardinality(files)\n",
    "            # Add one to the largest index to get the number of categories,\n",
    "            # since tf.nn.embedding_lookup expects zero-indexing. This\n",
    "            # means one or more at the bottom correspond to unused entries\n",
    "            # in the embedding lookup table. But that's a small waste of memory\n",
    "            # to keep the code simpler, and preserves correspondance between\n",
    "            # the id one specifies when generating, and the ids in the\n",
    "            # file names.\n",
    "            self.gc_category_cardinality += 1\n",
    "            print(\"Detected --gc_cardinality={}\".format(\n",
    "                  self.gc_category_cardinality))\n",
    "        else:\n",
    "            self.gc_category_cardinality = None\n",
    "\n",
    "    def dequeue(self, num_elements):\n",
    "        output = self.queue.dequeue_many(num_elements)\n",
    "        return output\n",
    "\n",
    "    def dequeue_gc(self, num_elements):\n",
    "        return self.gc_queue.dequeue_many(num_elements)\n",
    "\n",
    "    def thread_main(self, sess):\n",
    "        stop = False\n",
    "        # Go through the dataset multiple times\n",
    "        while not stop:\n",
    "            iterator = load_generic_audio(self.audio_dir, self.sample_rate)\n",
    "            for audio, filename, category_id in iterator:\n",
    "                if self.coord.should_stop():\n",
    "                    stop = True\n",
    "                    break\n",
    "                if self.silence_threshold is not None:\n",
    "                    # Remove silence\n",
    "                    audio = trim_silence(audio[:, 0], self.silence_threshold)\n",
    "                    audio = audio.reshape(-1, 1)\n",
    "                    if audio.size == 0:\n",
    "                        print(\"Warning: {} was ignored as it contains only \"\n",
    "                              \"silence. Consider decreasing trim_silence \"\n",
    "                              \"threshold, or adjust volume of the audio.\"\n",
    "                              .format(filename))\n",
    "\n",
    "                audio = np.pad(audio, [[self.receptive_field, 0], [0, 0]],\n",
    "                               'constant')\n",
    "\n",
    "                if self.sample_size:\n",
    "                    # Cut samples into pieces of size receptive_field +\n",
    "                    # sample_size with receptive_field overlap\n",
    "                    while len(audio) > self.receptive_field:\n",
    "                        piece = audio[:(self.receptive_field +\n",
    "                                        self.sample_size), :]\n",
    "                        sess.run(self.enqueue,\n",
    "                                 feed_dict={self.sample_placeholder: piece})\n",
    "                        audio = audio[self.sample_size:, :]\n",
    "                        if self.gc_enabled:\n",
    "                            sess.run(self.gc_enqueue, feed_dict={\n",
    "                                self.id_placeholder: category_id})\n",
    "                else:\n",
    "                    sess.run(self.enqueue,\n",
    "                             feed_dict={self.sample_placeholder: audio})\n",
    "                    if self.gc_enabled:\n",
    "                        sess.run(self.gc_enqueue,\n",
    "                                 feed_dict={self.id_placeholder: category_id})\n",
    "\n",
    "    def start_threads(self, sess, n_threads=1):\n",
    "        for _ in range(n_threads):\n",
    "            thread = threading.Thread(target=self.thread_main, args=(sess,))\n",
    "            thread.daemon = True  # Thread will close when parent quits.\n",
    "            thread.start()\n",
    "            self.threads.append(thread)\n",
    "        return self.threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_adam_optimizer(learning_rate, momentum):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                  epsilon=1e-4)\n",
    "\n",
    "\n",
    "def create_sgd_optimizer(learning_rate, momentum):\n",
    "    return tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=momentum)\n",
    "\n",
    "\n",
    "def create_rmsprop_optimizer(learning_rate, momentum):\n",
    "    return tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                     momentum=momentum,\n",
    "                                     epsilon=1e-5)\n",
    "\n",
    "\n",
    "optimizer_factory = {'adam': create_adam_optimizer,\n",
    "                     'sgd': create_sgd_optimizer,\n",
    "                     'rmsprop': create_rmsprop_optimizer}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proj3)",
   "language": "python",
   "name": "proj3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
