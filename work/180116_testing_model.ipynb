{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops.py\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_adam_optimizer(learning_rate, momentum):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                  epsilon=1e-4)\n",
    "\n",
    "\n",
    "def create_sgd_optimizer(learning_rate, momentum):\n",
    "    return tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=momentum)\n",
    "\n",
    "\n",
    "def create_rmsprop_optimizer(learning_rate, momentum):\n",
    "    return tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                     momentum=momentum,\n",
    "                                     epsilon=1e-5)\n",
    "\n",
    "\n",
    "optimizer_factory = {'adam': create_adam_optimizer,\n",
    "                     'sgd': create_sgd_optimizer,\n",
    "                     'rmsprop': create_rmsprop_optimizer}\n",
    "\n",
    "\n",
    "def time_to_batch(value, dilation, name=None):\n",
    "    with tf.name_scope('time_to_batch'):\n",
    "        shape = tf.shape(value)\n",
    "        pad_elements = dilation - 1 - (shape[1] + dilation - 1) % dilation\n",
    "        padded = tf.pad(value, [[0, 0], [0, pad_elements], [0, 0]])\n",
    "        reshaped = tf.reshape(padded, [-1, dilation, shape[2]])\n",
    "        transposed = tf.transpose(reshaped, perm=[1, 0, 2])\n",
    "        return tf.reshape(transposed, [shape[0] * dilation, -1, shape[2]])\n",
    "\n",
    "\n",
    "def batch_to_time(value, dilation, name=None):\n",
    "    with tf.name_scope('batch_to_time'):\n",
    "        shape = tf.shape(value)\n",
    "        prepared = tf.reshape(value, [dilation, -1, shape[2]])\n",
    "        transposed = tf.transpose(prepared, perm=[1, 0, 2])\n",
    "        return tf.reshape(transposed,\n",
    "                          [tf.div(shape[0], dilation), -1, shape[2]])\n",
    "\n",
    "\n",
    "def causal_conv(value, filter_, dilation, name='causal_conv'):\n",
    "    with tf.name_scope(name):\n",
    "        filter_width = tf.shape(filter_)[0]\n",
    "        if dilation > 1:\n",
    "            transformed = time_to_batch(value, dilation)\n",
    "            conv = tf.nn.conv1d(transformed, filter_, stride=1,\n",
    "                                padding='VALID')\n",
    "            restored = batch_to_time(conv, dilation)\n",
    "        else:\n",
    "            restored = tf.nn.conv1d(value, filter_, stride=1, padding='VALID')\n",
    "        # Remove excess elements at the end.\n",
    "        out_width = tf.shape(value)[1] - (filter_width - 1) * dilation\n",
    "        result = tf.slice(restored,\n",
    "                          [0, 0, 0],\n",
    "                          [-1, out_width, -1])\n",
    "        return result\n",
    "\n",
    "\n",
    "def mu_law_encode(audio, quantization_channels):\n",
    "    '''Quantizes waveform amplitudes.'''\n",
    "    with tf.name_scope('encode'):\n",
    "        mu = tf.to_float(quantization_channels - 1)\n",
    "        # Perform mu-law companding transformation (ITU-T, 1988).\n",
    "        # Minimum operation is here to deal with rare large amplitudes caused\n",
    "        # by resampling.\n",
    "        safe_audio_abs = tf.minimum(tf.abs(audio), 1.0)\n",
    "        magnitude = tf.log1p(mu * safe_audio_abs) / tf.log1p(mu)\n",
    "        signal = tf.sign(audio) * magnitude\n",
    "        # Quantize signal to the specified number of levels.\n",
    "        return tf.to_int32((signal + 1) / 2 * mu + 0.5)\n",
    "\n",
    "\n",
    "def mu_law_decode(output, quantization_channels):\n",
    "    '''Recovers waveform from quantized values.'''\n",
    "    with tf.name_scope('decode'):\n",
    "        mu = quantization_channels - 1\n",
    "        # Map values back to [-1, 1].\n",
    "        signal = 2 * (tf.to_float(output) / mu) - 1\n",
    "        # Perform inverse of mu-law transformation.\n",
    "        magnitude = (1 / mu) * ((1 + mu)**abs(signal) - 1)\n",
    "        return tf.sign(signal) * magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/aeatda/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/aeatda/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libnvidia-fatbinaryloader.so.387.26: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libnvidia-fatbinaryloader.so.387.26: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1222cdbd5d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcausal_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_law_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/aeatda/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/aeatda/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/aeatda/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libnvidia-fatbinaryloader.so.387.26: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# model.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from .ops import causal_conv, mu_law_encode\n",
    "\n",
    "\n",
    "def create_variable(name, shape):\n",
    "    '''Create a convolution filter variable with the specified name and shape,\n",
    "    and initialize it using Xavier initialition.'''\n",
    "    initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    variable = tf.Variable(initializer(shape=shape), name=name)\n",
    "    return variable\n",
    "\n",
    "\n",
    "def create_embedding_table(name, shape):\n",
    "    if shape[0] == shape[1]:\n",
    "        # Make a one-hot encoding as the initial value.\n",
    "        initial_val = np.identity(n=shape[0], dtype=np.float32)\n",
    "        return tf.Variable(initial_val, name=name)\n",
    "    else:\n",
    "        return create_variable(name, shape)\n",
    "\n",
    "\n",
    "def create_bias_variable(name, shape):\n",
    "    '''Create a bias variable with the specified name and shape and initialize\n",
    "    it to zero.'''\n",
    "    initializer = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    return tf.Variable(initializer(shape=shape), name)\n",
    "\n",
    "\n",
    "class WaveNetModel(object):\n",
    "    '''Implements the WaveNet network for generative audio.\n",
    "\n",
    "    Usage (with the architecture as in the DeepMind paper):\n",
    "        dilations = [2**i for i in range(N)] * M\n",
    "        filter_width = 2  # Convolutions just use 2 samples.\n",
    "        residual_channels = 16  # Not specified in the paper.\n",
    "        dilation_channels = 32  # Not specified in the paper.\n",
    "        skip_channels = 16      # Not specified in the paper.\n",
    "        net = WaveNetModel(batch_size, dilations, filter_width,\n",
    "                           residual_channels, dilation_channels,\n",
    "                           skip_channels)\n",
    "        loss = net.loss(input_batch)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 dilations,\n",
    "                 filter_width,\n",
    "                 residual_channels,\n",
    "                 dilation_channels,\n",
    "                 skip_channels,\n",
    "                 quantization_channels=2**8,\n",
    "                 use_biases=False,\n",
    "                 scalar_input=False,\n",
    "                 initial_filter_width=32,\n",
    "                 histograms=False,\n",
    "                 global_condition_channels=None,\n",
    "                 global_condition_cardinality=None):\n",
    "        '''Initializes the WaveNet model.\n",
    "\n",
    "        Args:\n",
    "            batch_size: How many audio files are supplied per batch\n",
    "                (recommended: 1).\n",
    "            dilations: A list with the dilation factor for each layer.\n",
    "            filter_width: The samples that are included in each convolution,\n",
    "                after dilating.\n",
    "            residual_channels: How many filters to learn for the residual.\n",
    "            dilation_channels: How many filters to learn for the dilated\n",
    "                convolution.\n",
    "            skip_channels: How many filters to learn that contribute to the\n",
    "                quantized softmax output.\n",
    "            quantization_channels: How many amplitude values to use for audio\n",
    "                quantization and the corresponding one-hot encoding.\n",
    "                Default: 256 (8-bit quantization).\n",
    "            use_biases: Whether to add a bias layer to each convolution.\n",
    "                Default: False.\n",
    "            scalar_input: Whether to use the quantized waveform directly as\n",
    "                input to the network instead of one-hot encoding it.\n",
    "                Default: False.\n",
    "            initial_filter_width: The width of the initial filter of the\n",
    "                convolution applied to the scalar input. This is only relevant\n",
    "                if scalar_input=True.\n",
    "            histograms: Whether to store histograms in the summary.\n",
    "                Default: False.\n",
    "            global_condition_channels: Number of channels in (embedding\n",
    "                size) of global conditioning vector. None indicates there is\n",
    "                no global conditioning.\n",
    "            global_condition_cardinality: Number of mutually exclusive\n",
    "                categories to be embedded in global condition embedding. If\n",
    "                not None, then this implies that global_condition tensor\n",
    "                specifies an integer selecting which of the N global condition\n",
    "                categories, where N = global_condition_cardinality. If None,\n",
    "                then the global_condition tensor is regarded as a vector which\n",
    "                must have dimension global_condition_channels.\n",
    "\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_width = filter_width\n",
    "        self.residual_channels = residual_channels\n",
    "        self.dilation_channels = dilation_channels\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.use_biases = use_biases\n",
    "        self.skip_channels = skip_channels\n",
    "        self.scalar_input = scalar_input\n",
    "        self.initial_filter_width = initial_filter_width\n",
    "        self.histograms = histograms\n",
    "        self.global_condition_channels = global_condition_channels\n",
    "        self.global_condition_cardinality = global_condition_cardinality\n",
    "\n",
    "        self.receptive_field = WaveNetModel.calculate_receptive_field(\n",
    "            self.filter_width, self.dilations, self.scalar_input,\n",
    "            self.initial_filter_width)\n",
    "        self.variables = self._create_variables()\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_receptive_field(filter_width, dilations, scalar_input,\n",
    "                                  initial_filter_width):\n",
    "        receptive_field = (filter_width - 1) * sum(dilations) + 1\n",
    "        if scalar_input:\n",
    "            receptive_field += initial_filter_width - 1\n",
    "        else:\n",
    "            receptive_field += filter_width - 1\n",
    "        return receptive_field\n",
    "\n",
    "    def _create_variables(self):\n",
    "        '''This function creates all variables used by the network.\n",
    "        This allows us to share them between multiple calls to the loss\n",
    "        function and generation function.'''\n",
    "\n",
    "        var = dict()\n",
    "\n",
    "        with tf.variable_scope('wavenet'):\n",
    "            if self.global_condition_cardinality is not None:\n",
    "                # We only look up the embedding if we are conditioning on a\n",
    "                # set of mutually-exclusive categories. We can also condition\n",
    "                # on an already-embedded dense vector, in which case it's\n",
    "                # given to us and we don't need to do the embedding lookup.\n",
    "                # Still another alternative is no global condition at all, in\n",
    "                # which case we also don't do a tf.nn.embedding_lookup.\n",
    "                with tf.variable_scope('embeddings'):\n",
    "                    layer = dict()\n",
    "                    layer['gc_embedding'] = create_embedding_table(\n",
    "                        'gc_embedding',\n",
    "                        [self.global_condition_cardinality,\n",
    "                         self.global_condition_channels])\n",
    "                    var['embeddings'] = layer\n",
    "\n",
    "            with tf.variable_scope('causal_layer'):\n",
    "                layer = dict()\n",
    "                if self.scalar_input:\n",
    "                    initial_channels = 1\n",
    "                    initial_filter_width = self.initial_filter_width\n",
    "                else:\n",
    "                    initial_channels = self.quantization_channels\n",
    "                    initial_filter_width = self.filter_width\n",
    "                layer['filter'] = create_variable(\n",
    "                    'filter',\n",
    "                    [initial_filter_width,\n",
    "                     initial_channels,\n",
    "                     self.residual_channels])\n",
    "                var['causal_layer'] = layer\n",
    "\n",
    "            var['dilated_stack'] = list()\n",
    "            with tf.variable_scope('dilated_stack'):\n",
    "                for i, dilation in enumerate(self.dilations):\n",
    "                    with tf.variable_scope('layer{}'.format(i)):\n",
    "                        current = dict()\n",
    "                        current['filter'] = create_variable(\n",
    "                            'filter',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['gate'] = create_variable(\n",
    "                            'gate',\n",
    "                            [self.filter_width,\n",
    "                             self.residual_channels,\n",
    "                             self.dilation_channels])\n",
    "                        current['dense'] = create_variable(\n",
    "                            'dense',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.residual_channels])\n",
    "                        current['skip'] = create_variable(\n",
    "                            'skip',\n",
    "                            [1,\n",
    "                             self.dilation_channels,\n",
    "                             self.skip_channels])\n",
    "\n",
    "                        if self.global_condition_channels is not None:\n",
    "                            current['gc_gateweights'] = create_variable(\n",
    "                                'gc_gate',\n",
    "                                [1, self.global_condition_channels,\n",
    "                                 self.dilation_channels])\n",
    "                            current['gc_filtweights'] = create_variable(\n",
    "                                'gc_filter',\n",
    "                                [1, self.global_condition_channels,\n",
    "                                 self.dilation_channels])\n",
    "\n",
    "                        if self.use_biases:\n",
    "                            current['filter_bias'] = create_bias_variable(\n",
    "                                'filter_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['gate_bias'] = create_bias_variable(\n",
    "                                'gate_bias',\n",
    "                                [self.dilation_channels])\n",
    "                            current['dense_bias'] = create_bias_variable(\n",
    "                                'dense_bias',\n",
    "                                [self.residual_channels])\n",
    "                            current['skip_bias'] = create_bias_variable(\n",
    "                                'slip_bias',\n",
    "                                [self.skip_channels])\n",
    "\n",
    "                        var['dilated_stack'].append(current)\n",
    "\n",
    "            with tf.variable_scope('postprocessing'):\n",
    "                current = dict()\n",
    "                current['postprocess1'] = create_variable(\n",
    "                    'postprocess1',\n",
    "                    [1, self.skip_channels, self.skip_channels])\n",
    "                current['postprocess2'] = create_variable(\n",
    "                    'postprocess2',\n",
    "                    [1, self.skip_channels, self.quantization_channels])\n",
    "                if self.use_biases:\n",
    "                    current['postprocess1_bias'] = create_bias_variable(\n",
    "                        'postprocess1_bias',\n",
    "                        [self.skip_channels])\n",
    "                    current['postprocess2_bias'] = create_bias_variable(\n",
    "                        'postprocess2_bias',\n",
    "                        [self.quantization_channels])\n",
    "                var['postprocessing'] = current\n",
    "\n",
    "        return var\n",
    "\n",
    "    def _create_causal_layer(self, input_batch):\n",
    "        '''Creates a single causal convolution layer.\n",
    "\n",
    "        The layer can change the number of channels.\n",
    "        '''\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            return causal_conv(input_batch, weights_filter, 1)\n",
    "\n",
    "    def _create_dilation_layer(self, input_batch, layer_index, dilation,\n",
    "                               global_condition_batch, output_width):\n",
    "        '''Creates a single causal dilated convolution layer.\n",
    "\n",
    "        Args:\n",
    "             input_batch: Input to the dilation layer.\n",
    "             layer_index: Integer indicating which layer this is.\n",
    "             dilation: Integer specifying the dilation size.\n",
    "             global_conditioning_batch: Tensor containing the global data upon\n",
    "                 which the output is to be conditioned upon. Shape:\n",
    "                 [batch size, 1, channels]. The 1 is for the axis\n",
    "                 corresponding to time so that the result is broadcast to\n",
    "                 all time steps.\n",
    "\n",
    "        The layer contains a gated filter that connects to dense output\n",
    "        and to a skip connection:\n",
    "\n",
    "               |-> [gate]   -|        |-> 1x1 conv -> skip output\n",
    "               |             |-> (*) -|\n",
    "        input -|-> [filter] -|        |-> 1x1 conv -|\n",
    "               |                                    |-> (+) -> dense output\n",
    "               |------------------------------------|\n",
    "\n",
    "        Where `[gate]` and `[filter]` are causal convolutions with a\n",
    "        non-linear activation at the output. Biases and global conditioning\n",
    "        are omitted due to the limits of ASCII art.\n",
    "\n",
    "        '''\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "\n",
    "        conv_filter = causal_conv(input_batch, weights_filter, dilation)\n",
    "        conv_gate = causal_conv(input_batch, weights_gate, dilation)\n",
    "\n",
    "        if global_condition_batch is not None:\n",
    "            weights_gc_filter = variables['gc_filtweights']\n",
    "            conv_filter = conv_filter + tf.nn.conv1d(global_condition_batch,\n",
    "                                                     weights_gc_filter,\n",
    "                                                     stride=1,\n",
    "                                                     padding=\"SAME\",\n",
    "                                                     name=\"gc_filter\")\n",
    "            weights_gc_gate = variables['gc_gateweights']\n",
    "            conv_gate = conv_gate + tf.nn.conv1d(global_condition_batch,\n",
    "                                                 weights_gc_gate,\n",
    "                                                 stride=1,\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 name=\"gc_gate\")\n",
    "\n",
    "        if self.use_biases:\n",
    "            filter_bias = variables['filter_bias']\n",
    "            gate_bias = variables['gate_bias']\n",
    "            conv_filter = tf.add(conv_filter, filter_bias)\n",
    "            conv_gate = tf.add(conv_gate, gate_bias)\n",
    "\n",
    "        out = tf.tanh(conv_filter) * tf.sigmoid(conv_gate)\n",
    "\n",
    "        # The 1x1 conv to produce the residual output\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.nn.conv1d(\n",
    "            out, weights_dense, stride=1, padding=\"SAME\", name=\"dense\")\n",
    "\n",
    "        # The 1x1 conv to produce the skip output\n",
    "        skip_cut = tf.shape(out)[1] - output_width\n",
    "        out_skip = tf.slice(out, [0, skip_cut, 0], [-1, -1, -1])\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.nn.conv1d(\n",
    "            out_skip, weights_skip, stride=1, padding=\"SAME\", name=\"skip\")\n",
    "\n",
    "        if self.use_biases:\n",
    "            dense_bias = variables['dense_bias']\n",
    "            skip_bias = variables['skip_bias']\n",
    "            transformed = transformed + dense_bias\n",
    "            skip_contribution = skip_contribution + skip_bias\n",
    "\n",
    "        if self.histograms:\n",
    "            layer = 'layer{}'.format(layer_index)\n",
    "            tf.histogram_summary(layer + '_filter', weights_filter)\n",
    "            tf.histogram_summary(layer + '_gate', weights_gate)\n",
    "            tf.histogram_summary(layer + '_dense', weights_dense)\n",
    "            tf.histogram_summary(layer + '_skip', weights_skip)\n",
    "            if self.use_biases:\n",
    "                tf.histogram_summary(layer + '_biases_filter', filter_bias)\n",
    "                tf.histogram_summary(layer + '_biases_gate', gate_bias)\n",
    "                tf.histogram_summary(layer + '_biases_dense', dense_bias)\n",
    "                tf.histogram_summary(layer + '_biases_skip', skip_bias)\n",
    "\n",
    "        input_cut = tf.shape(input_batch)[1] - tf.shape(transformed)[1]\n",
    "        input_batch = tf.slice(input_batch, [0, input_cut, 0], [-1, -1, -1])\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _generator_conv(self, input_batch, state_batch, weights):\n",
    "        '''Perform convolution for a single convolutional processing step.'''\n",
    "        # TODO generalize to filter_width > 2\n",
    "        past_weights = weights[0, :, :]\n",
    "        curr_weights = weights[1, :, :]\n",
    "        output = tf.matmul(state_batch, past_weights) + tf.matmul(\n",
    "            input_batch, curr_weights)\n",
    "        return output\n",
    "\n",
    "    def _generator_causal_layer(self, input_batch, state_batch):\n",
    "        with tf.name_scope('causal_layer'):\n",
    "            weights_filter = self.variables['causal_layer']['filter']\n",
    "            output = self._generator_conv(\n",
    "                input_batch, state_batch, weights_filter)\n",
    "        return output\n",
    "\n",
    "    def _generator_dilation_layer(self, input_batch, state_batch, layer_index,\n",
    "                                  dilation, global_condition_batch):\n",
    "        variables = self.variables['dilated_stack'][layer_index]\n",
    "\n",
    "        weights_filter = variables['filter']\n",
    "        weights_gate = variables['gate']\n",
    "        output_filter = self._generator_conv(\n",
    "            input_batch, state_batch, weights_filter)\n",
    "        output_gate = self._generator_conv(\n",
    "            input_batch, state_batch, weights_gate)\n",
    "\n",
    "        if global_condition_batch is not None:\n",
    "            global_condition_batch = tf.reshape(global_condition_batch,\n",
    "                                                shape=(1, -1))\n",
    "            weights_gc_filter = variables['gc_filtweights']\n",
    "            weights_gc_filter = weights_gc_filter[0, :, :]\n",
    "            output_filter += tf.matmul(global_condition_batch,\n",
    "                                       weights_gc_filter)\n",
    "            weights_gc_gate = variables['gc_gateweights']\n",
    "            weights_gc_gate = weights_gc_gate[0, :, :]\n",
    "            output_gate += tf.matmul(global_condition_batch,\n",
    "                                     weights_gc_gate)\n",
    "\n",
    "        if self.use_biases:\n",
    "            output_filter = output_filter + variables['filter_bias']\n",
    "            output_gate = output_gate + variables['gate_bias']\n",
    "\n",
    "        out = tf.tanh(output_filter) * tf.sigmoid(output_gate)\n",
    "\n",
    "        weights_dense = variables['dense']\n",
    "        transformed = tf.matmul(out, weights_dense[0, :, :])\n",
    "        if self.use_biases:\n",
    "            transformed = transformed + variables['dense_bias']\n",
    "\n",
    "        weights_skip = variables['skip']\n",
    "        skip_contribution = tf.matmul(out, weights_skip[0, :, :])\n",
    "        if self.use_biases:\n",
    "            skip_contribution = skip_contribution + variables['skip_bias']\n",
    "\n",
    "        return skip_contribution, input_batch + transformed\n",
    "\n",
    "    def _create_network(self, input_batch, global_condition_batch):\n",
    "        '''Construct the WaveNet network.'''\n",
    "        outputs = []\n",
    "        current_layer = input_batch\n",
    "\n",
    "        # Pre-process the input with a regular convolution\n",
    "        if self.scalar_input:\n",
    "            initial_channels = 1\n",
    "        else:\n",
    "            initial_channels = self.quantization_channels\n",
    "\n",
    "        current_layer = self._create_causal_layer(current_layer)\n",
    "\n",
    "        output_width = tf.shape(input_batch)[1] - self.receptive_field + 1\n",
    "\n",
    "        # Add all defined dilation layers.\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                    output, current_layer = self._create_dilation_layer(\n",
    "                        current_layer, layer_index, dilation,\n",
    "                        global_condition_batch, output_width)\n",
    "                    outputs.append(output)\n",
    "\n",
    "        with tf.name_scope('postprocessing'):\n",
    "            # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "            # postprocess the output.\n",
    "            w1 = self.variables['postprocessing']['postprocess1']\n",
    "            w2 = self.variables['postprocessing']['postprocess2']\n",
    "            if self.use_biases:\n",
    "                b1 = self.variables['postprocessing']['postprocess1_bias']\n",
    "                b2 = self.variables['postprocessing']['postprocess2_bias']\n",
    "\n",
    "            if self.histograms:\n",
    "                tf.histogram_summary('postprocess1_weights', w1)\n",
    "                tf.histogram_summary('postprocess2_weights', w2)\n",
    "                if self.use_biases:\n",
    "                    tf.histogram_summary('postprocess1_biases', b1)\n",
    "                    tf.histogram_summary('postprocess2_biases', b2)\n",
    "\n",
    "            # We skip connections from the outputs of each layer, adding them\n",
    "            # all up here.\n",
    "            total = sum(outputs)\n",
    "            transformed1 = tf.nn.relu(total)\n",
    "            conv1 = tf.nn.conv1d(transformed1, w1, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv1 = tf.add(conv1, b1)\n",
    "            transformed2 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.nn.conv1d(transformed2, w2, stride=1, padding=\"SAME\")\n",
    "            if self.use_biases:\n",
    "                conv2 = tf.add(conv2, b2)\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    def _create_generator(self, input_batch, global_condition_batch):\n",
    "        '''Construct an efficient incremental generator.'''\n",
    "        init_ops = []\n",
    "        push_ops = []\n",
    "        outputs = []\n",
    "        current_layer = input_batch\n",
    "\n",
    "        q = tf.FIFOQueue(\n",
    "            1,\n",
    "            dtypes=tf.float32,\n",
    "            shapes=(self.batch_size, self.quantization_channels))\n",
    "        init = q.enqueue_many(\n",
    "            tf.zeros((1, self.batch_size, self.quantization_channels)))\n",
    "\n",
    "        current_state = q.dequeue()\n",
    "        push = q.enqueue([current_layer])\n",
    "        init_ops.append(init)\n",
    "        push_ops.append(push)\n",
    "\n",
    "        current_layer = self._generator_causal_layer(\n",
    "                            current_layer, current_state)\n",
    "\n",
    "        # Add all defined dilation layers.\n",
    "        with tf.name_scope('dilated_stack'):\n",
    "            for layer_index, dilation in enumerate(self.dilations):\n",
    "                with tf.name_scope('layer{}'.format(layer_index)):\n",
    "\n",
    "                    q = tf.FIFOQueue(\n",
    "                        dilation,\n",
    "                        dtypes=tf.float32,\n",
    "                        shapes=(self.batch_size, self.residual_channels))\n",
    "                    init = q.enqueue_many(\n",
    "                        tf.zeros((dilation, self.batch_size,\n",
    "                                  self.residual_channels)))\n",
    "\n",
    "                    current_state = q.dequeue()\n",
    "                    push = q.enqueue([current_layer])\n",
    "                    init_ops.append(init)\n",
    "                    push_ops.append(push)\n",
    "\n",
    "                    output, current_layer = self._generator_dilation_layer(\n",
    "                        current_layer, current_state, layer_index, dilation,\n",
    "                        global_condition_batch)\n",
    "                    outputs.append(output)\n",
    "        self.init_ops = init_ops\n",
    "        self.push_ops = push_ops\n",
    "\n",
    "        with tf.name_scope('postprocessing'):\n",
    "            variables = self.variables['postprocessing']\n",
    "            # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "            # postprocess the output.\n",
    "            w1 = variables['postprocess1']\n",
    "            w2 = variables['postprocess2']\n",
    "            if self.use_biases:\n",
    "                b1 = variables['postprocess1_bias']\n",
    "                b2 = variables['postprocess2_bias']\n",
    "\n",
    "            # We skip connections from the outputs of each layer, adding them\n",
    "            # all up here.\n",
    "            total = sum(outputs)\n",
    "            transformed1 = tf.nn.relu(total)\n",
    "\n",
    "            conv1 = tf.matmul(transformed1, w1[0, :, :])\n",
    "            if self.use_biases:\n",
    "                conv1 = conv1 + b1\n",
    "            transformed2 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.matmul(transformed2, w2[0, :, :])\n",
    "            if self.use_biases:\n",
    "                conv2 = conv2 + b2\n",
    "\n",
    "        return conv2\n",
    "\n",
    "    def _one_hot(self, input_batch):\n",
    "        '''One-hot encodes the waveform amplitudes.\n",
    "\n",
    "        This allows the definition of the network as a categorical distribution\n",
    "        over a finite set of possible amplitudes.\n",
    "        '''\n",
    "        with tf.name_scope('one_hot_encode'):\n",
    "            encoded = tf.one_hot(\n",
    "                input_batch,\n",
    "                depth=self.quantization_channels,\n",
    "                dtype=tf.float32)\n",
    "            shape = [self.batch_size, -1, self.quantization_channels]\n",
    "            encoded = tf.reshape(encoded, shape)\n",
    "        return encoded\n",
    "\n",
    "    def _embed_gc(self, global_condition):\n",
    "        '''Returns embedding for global condition.\n",
    "        :param global_condition: Either ID of global condition for\n",
    "               tf.nn.embedding_lookup or actual embedding. The latter is\n",
    "               experimental.\n",
    "        :return: Embedding or None\n",
    "        '''\n",
    "        embedding = None\n",
    "        if self.global_condition_cardinality is not None:\n",
    "            # Only lookup the embedding if the global condition is presented\n",
    "            # as an integer of mutually-exclusive categories ...\n",
    "            embedding_table = self.variables['embeddings']['gc_embedding']\n",
    "            embedding = tf.nn.embedding_lookup(embedding_table,\n",
    "                                               global_condition)\n",
    "        elif global_condition is not None:\n",
    "            # ... else the global_condition (if any) is already provided\n",
    "            # as an embedding.\n",
    "\n",
    "            # In this case, the number of global_embedding channels must be\n",
    "            # equal to the the last dimension of the global_condition tensor.\n",
    "            gc_batch_rank = len(global_condition.get_shape())\n",
    "            dims_match = (global_condition.get_shape()[gc_batch_rank - 1] ==\n",
    "                          self.global_condition_channels)\n",
    "            if not dims_match:\n",
    "                raise ValueError('Shape of global_condition {} does not'\n",
    "                                 ' match global_condition_channels {}.'.\n",
    "                                 format(global_condition.get_shape(),\n",
    "                                        self.global_condition_channels))\n",
    "            embedding = global_condition\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding = tf.reshape(\n",
    "                embedding,\n",
    "                [self.batch_size, 1, self.global_condition_channels])\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    def predict_proba(self, waveform, global_condition=None, name='wavenet'):\n",
    "        '''Computes the probability distribution of the next sample based on\n",
    "        all samples in the input waveform.\n",
    "        If you want to generate audio by feeding the output of the network back\n",
    "        as an input, see predict_proba_incremental for a faster alternative.'''\n",
    "        with tf.name_scope(name):\n",
    "            if self.scalar_input:\n",
    "                encoded = tf.cast(waveform, tf.float32)\n",
    "                encoded = tf.reshape(encoded, [-1, 1])\n",
    "            else:\n",
    "                encoded = self._one_hot(waveform)\n",
    "\n",
    "            gc_embedding = self._embed_gc(global_condition)\n",
    "            raw_output = self._create_network(encoded, gc_embedding)\n",
    "            out = tf.reshape(raw_output, [-1, self.quantization_channels])\n",
    "            # Cast to float64 to avoid bug in TensorFlow\n",
    "            proba = tf.cast(\n",
    "                tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n",
    "            last = tf.slice(\n",
    "                proba,\n",
    "                [tf.shape(proba)[0] - 1, 0],\n",
    "                [1, self.quantization_channels])\n",
    "            return tf.reshape(last, [-1])\n",
    "\n",
    "    def predict_proba_incremental(self, waveform, global_condition=None,\n",
    "                                  name='wavenet'):\n",
    "        '''Computes the probability distribution of the next sample\n",
    "        incrementally, based on a single sample and all previously passed\n",
    "        samples.'''\n",
    "        if self.filter_width > 2:\n",
    "            raise NotImplementedError(\"Incremental generation does not \"\n",
    "                                      \"support filter_width > 2.\")\n",
    "        if self.scalar_input:\n",
    "            raise NotImplementedError(\"Incremental generation does not \"\n",
    "                                      \"support scalar input yet.\")\n",
    "        with tf.name_scope(name):\n",
    "            encoded = tf.one_hot(waveform, self.quantization_channels)\n",
    "            encoded = tf.reshape(encoded, [-1, self.quantization_channels])\n",
    "            gc_embedding = self._embed_gc(global_condition)\n",
    "            raw_output = self._create_generator(encoded, gc_embedding)\n",
    "            out = tf.reshape(raw_output, [-1, self.quantization_channels])\n",
    "            proba = tf.cast(\n",
    "                tf.nn.softmax(tf.cast(out, tf.float64)), tf.float32)\n",
    "            last = tf.slice(\n",
    "                proba,\n",
    "                [tf.shape(proba)[0] - 1, 0],\n",
    "                [1, self.quantization_channels])\n",
    "            return tf.reshape(last, [-1])\n",
    "\n",
    "    def loss(self,\n",
    "             input_batch,\n",
    "             global_condition_batch=None,\n",
    "             l2_regularization_strength=None,\n",
    "             name='wavenet'):\n",
    "        '''Creates a WaveNet network and returns the autoencoding loss.\n",
    "\n",
    "        The variables are all scoped to the given name.\n",
    "        '''\n",
    "        with tf.name_scope(name):\n",
    "            # We mu-law encode and quantize the input audioform.\n",
    "            encoded_input = mu_law_encode(input_batch,\n",
    "                                          self.quantization_channels)\n",
    "\n",
    "            gc_embedding = self._embed_gc(global_condition_batch)\n",
    "            encoded = self._one_hot(encoded_input)\n",
    "            if self.scalar_input:\n",
    "                network_input = tf.reshape(\n",
    "                    tf.cast(input_batch, tf.float32),\n",
    "                    [self.batch_size, -1, 1])\n",
    "            else:\n",
    "                network_input = encoded\n",
    "\n",
    "            # Cut off the last sample of network input to preserve causality.\n",
    "            network_input_width = tf.shape(network_input)[1] - 1\n",
    "            network_input = tf.slice(network_input, [0, 0, 0],\n",
    "                                     [-1, network_input_width, -1])\n",
    "\n",
    "            raw_output = self._create_network(network_input, gc_embedding)\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "                # Cut off the samples corresponding to the receptive field\n",
    "                # for the first predicted sample.\n",
    "                target_output = tf.slice(\n",
    "                    tf.reshape(\n",
    "                        encoded,\n",
    "                        [self.batch_size, -1, self.quantization_channels]),\n",
    "                    [0, self.receptive_field, 0],\n",
    "                    [-1, -1, -1])\n",
    "                target_output = tf.reshape(target_output,\n",
    "                                           [-1, self.quantization_channels])\n",
    "                prediction = tf.reshape(raw_output,\n",
    "                                        [-1, self.quantization_channels])\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=prediction,\n",
    "                    labels=target_output)\n",
    "                reduced_loss = tf.reduce_mean(loss)\n",
    "\n",
    "                tf.summary.scalar('loss', reduced_loss)\n",
    "\n",
    "                if l2_regularization_strength is None:\n",
    "                    return reduced_loss\n",
    "                else:\n",
    "                    # L2 regularization for all trainable parameters\n",
    "                    l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                        for v in tf.trainable_variables()\n",
    "                                        if not('bias' in v.name)])\n",
    "\n",
    "                    # Add the regularization term to the loss\n",
    "                    total_loss = (reduced_loss +\n",
    "                                  l2_regularization_strength * l2_loss)\n",
    "\n",
    "                    tf.summary.scalar('l2_loss', l2_loss)\n",
    "                    tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "                    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
